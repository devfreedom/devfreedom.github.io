<!DOCTYPE html>
<html>
    
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>[개발 일기] 머신러닝 멘토링 세션 - PyTorch로 감정 분류기 개발하기 | devfreedom.github.io</title>
    <meta name="title" content="[개발 일기] 머신러닝 멘토링 세션 - PyTorch로 감정 분류기 개발하기 | devfreedom.github.io">
    <meta name="description" content="devfreedom&#39;s personal blog">
    <meta name="keywords" content="❮개발 일기❯,인공지능,머신러닝,Python,PyTorch,백엔드,자연어 처리,eleventy,template,simple,clean">
    <meta name="author" content="devfreedom">
    <meta name="robots" content="index, follow">
    <link rel="canonical" href="https://devfreedom.github.io/20240201_1912_dev-diary_ai_ml_nlp_mentoring/">
    <link rel="shortcut icon" type="image/png" href="/assets/img/favicon.svg">
    <link rel="apple-touch-icon" href="/assets/img/apple-touch-icon.png">
    <link rel="dns-prefetch" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="dns-prefetch" href="https://fonts.gstatic.com">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap">
    <link rel="stylesheet" href="/assets/css/main.css">
    
</head>	

<body class="flex flex-col h-screen bg-white text-gray-800 break-words">
    <header id="header" class="header-shadow bg-black px-6 py-5 z-50 fixed w-full top-0 transition-all transform ease-in-out duration-500">
    <div class="max-w-5xl mx-auto flex items-center flex-wrap justify-between">
        <div class="sm:mr-8">
            <a class="flex items-center" href="/">                              
                <span class="text-lg text-white font-semibold self-center">devfreedom.github.io</span>
            </a>
        </div>
        <nav id="menu" class="order-last md:order-none items-center flex-grow w-full md:w-auto md:flex hidden mt-2 md:mt-0">
            
            <a href="/tags" class="block mt-4 md:inline-block md:mt-0 font-medium text-gray-500 hover:text-white text-base mr-7">Categories</a>
            
            <a href="https://github.com/devfreedom" target="_blank" rel="noopener" class="block mt-4 md:inline-block md:mt-0 font-medium text-gray-500 hover:text-white text-base mr-7">GitHub</a>
            
            <a href="/about" class="block mt-4 md:inline-block md:mt-0 font-medium text-gray-500 hover:text-white text-base mr-7">About</a>
            
        </nav>
        <form id="search" action="/search" class="order-last sm:order-none flex-auto w-32 items-center justify-end hidden sm:block mt-6 sm:mt-0">
            <label class="visually-hidden" for="header-searchbox">Search here ...</label>
            <input type="text" id="header-searchbox" name="q" placeholder="Search..." class="w-full sm:max-w-xs bg-gray-200 border border-transparent float-right focus:bg-white focus:border-gray-300 focus:outline-none h-8 p-4 placeholder-gray-700 rounded text-gray-700 text-sm">
        </form>
        <div id="menu-toggle" class="flex items-center md:hidden text-gray-700 hover:text-teal-600 cursor-pointer sm:ml-6">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
    <line x1="3" y1="12" x2="21" y2="12"></line>
    <line x1="3" y1="6" x2="21" y2="6"></line>
    <line x1="3" y1="18" x2="21" y2="18"></line>
</svg>
        </div>
    </div>
</header>
    <main class="mx-7 lg:mx-6 mt-32 flex-grow">
        
        
<article class="max-w-5xl mx-auto">
    <header class="mb-14">
        <h1 class="text-3xl text-center font-bold leading-normal text-gray-900 mt-0 mb-3">[개발 일기] 머신러닝 멘토링 세션 - PyTorch로 감정 분류기 개발하기</h1>
        <div class="text-center">Published on 1 February 2024 07:12 PM</div>
        
        <div class="mt-3 text-center">
            
            <a href="/tags/❮개발 일기❯" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#❮개발 일기❯</a>
            
            <a href="/tags/인공지능" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#인공지능</a>
            
            <a href="/tags/머신러닝" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#머신러닝</a>
            
            <a href="/tags/Python" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#Python</a>
            
            <a href="/tags/PyTorch" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#PyTorch</a>
            
            <a href="/tags/백엔드" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#백엔드</a>
            
            <a href="/tags/자연어 처리" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#자연어 처리</a>
            
        </div>
        
        
        <div class="mt-10 -mx-7 md:mx-0">
            <img class="w-full max-w-2xl mx-auto" src="/assets/img/neural-network.jpg" width="960" height="500" alt="This post thumbnail">
        </div>
        
    </header>
    <div id="content" class="prose text-gray-800 max-w-none">
        <p>자연어 처리 머신러닝을 기반으로 감정 분류기를 개발하는 과정에서 현직자 멘토께서 조언해주신 내용을 정리해봤습니다.</p>
<hr>
<h1>브레인스토밍</h1>
<h3>Q. 사용자가 일기를 작성하면 그 일기의 감정을 파악해 '기쁨', '슬픔', '즐거움' 등으로 자동 분류를 하고자 합니다. 머신러닝을 어떻게 활용할 수 있을까요?</h3>
<p>A. 두 가지 방법이 있을 수 있겠네요.</p>
<ol>
<li>모델로 학습해서 사용하는 방법이 있습니다.
<ul>
<li>Word embedding
<ul>
<li>예를 들면 '일출'이라는 단어가 있을 때 단어의 벡터를 추출해 그것과 연관된 '해돋이'라는 단어를 유사도 기반으로 연결하는 vectorization 모델이 있습니다.
<ul>
<li>e.g. Word2Vec, fastText</li>
</ul>
</li>
<li>'후처리 사전'을 만든다고 생각하시면 됩니다.</li>
</ul>
</li>
<li>모델 종류
<ul>
<li>Interpretable / Explainable 모델
<ul>
<li>분류 모델 + Local Interpretable Model-Agnostic Explanation (LIME)</li>
<li>구현 난이도가 높으므로 여기서는 고려하지 않도록 하겠습니다.</li>
</ul>
</li>
<li>NER 모델
<ul>
<li>Named-entity recognition은 일종의 형태소 분석기라고 생각하시면 됩니다.</li>
<li>e.g. ['오늘', '비', '가', '왔다'] → [0, '슬픔', 0, 0]</li>
</ul>
</li>
</ul>
</li>
<li>장점
<ul>
<li>구현이 간단합니다.</li>
<li>모델 학습 속도도 빠르고 사전 학습된 모델들도 많습니다.</li>
</ul>
</li>
<li>단점
<ul>
<li>문맥적인 정확도가 낮을 수 있습니다.
<ul>
<li>'일출'을 예로 들면 '해돋이'가 아닌 '일몰' 역시 유사도가 높게 나올 수 있습니다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>사람이 직접 '단어-감정' pair를 태깅/매핑을 하는 방법이 있습니다.
<ul>
<li>과정
<ol>
<li>먼저 문장에 tokenize 과정을 수행합니다.</li>
<li>각 토큰에 감정을 태깅합니다.
<ul>
<li>단어에 감정을 태깅하는 작업에는 Amazon Groundtruth 등의 툴을 활용할 수 있습니다.</li>
</ul>
</li>
</ol>
</li>
<li>장점
<ul>
<li>정공법으로써 성능도 높고 정확도도 뛰어납니다.</li>
</ul>
</li>
<li>단점
<ul>
<li>공개된 데이터 소스를 구하고, 태깅으로 데이터를 구축하는 데 상당한 시간과 노력이 소요됩니다.</li>
<li>'차원의 저주' 현상
<ul>
<li>데이터 학습을 위해 '공간의 차원이 증가'하지만 학습 데이터가 부족하면 '데이터의 밀도'가 급격히 감소하고, 이로 인해 데이터 분석이나 머신러닝 모델의 성능에 부정적인 영향을 미치는 '차원의 저주' 현상이 발생할 수 있습니다.</li>
<li>분류할 감정이 몇 종류인지 역시 중요합니다.
<ul>
<li>만약 분류될 클래스가 100개라면 적어도 10000개의 학습 데이터가 필요하고, 그만큼 수동으로 태깅을 해주어야 하므로 상당한 시간과 노력이 필요합니다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h3>Q. KoBERT를 활용할 수는 없을까요?</h3>
<p>A. KoBERT는 '한국어에 대한 이해'만을 가지고 있으므로, 이를 '감정 분류'에 사용하려면 학습을 한번 더 시켜줘야 합니다.</p>
<ul>
<li>KoBERT는 한국어 기반의 Bidirectional Encoder Representations from Transformers (BERT) 모델입니다.
<ul>
<li>다음과 같은 사전 학습을 이미 거친 pre-trained 모델입니다.
<ul>
<li>e.g. ['오늘', '날씨', '가', '참', '덥', '네요']
<ol>
<li>토큰의 일부를 마스킹 처리합니다.
<ul>
<li>['오늘', '날씨', &quot;masked&quot;, '참', &quot;masked&quot;, '네요']</li>
</ul>
</li>
<li>마스킹된 토큰을 예측합니다.
<ul>
<li>{'날씨': '가', '참': '덥'}</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li>분류, 번역, QA, summarization 등으로 fine-tuning을 진행합니다.</li>
</ul>
</li>
<li>여기서는 모델로 학습을 하되, 정확도를 human review로 보정하는 방식을 고려해볼 수 있습니다.</li>
</ul>
<h3>Q. 머신러닝 서비스를 웹 서비스와 어떻게 연동할까요?</h3>
<p>A. 단기간에 빠르게 개발하기에는 Flask가 좋습니다.</p>
<ul>
<li>TensorFlow나 Keras는 자바스크립트로도 사용할 수 있는 방법이 있는 것으로 알고 있습니다.</li>
<li>하지만 현업에서 대세로 사용되는 머신러닝 프레임워크는 PyTorch이므로 파이썬 기반의 서버를 구축하는 것이 좋습니다.</li>
</ul>
<h3>Q. 감정 분류용 머신러닝 백엔드 처리 구조를 어떻게 설정하는 것이 좋을까요?</h3>
<ol>
<li>사용자가 일기를 작성함
<ul>
<li>→ 웹 서비스 백엔드가 일기 데이터를 받아 DB에 저장
<ul>
<li>→ 머신러닝 서비스 백엔드가 DB로부터 일기 데이터를 가져와서 감정 분류를 별도로 수행
<ul>
<li>→ 머신러닝 서비스 백엔드가 감정 분류 결과를 DB에 저장</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>사용자가 일기를 작성함
<ul>
<li>→ 머신러닝 서비스 백엔드가 이를 받아 감정 분류를 먼저 수행
<ul>
<li>→ 머신러닝 서비스 백엔드가 일기 데이터와 감정 분류 결과를 DB에 함께 저장</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>A. 이건 백엔드 설계 이슈에 가깝고, 두 구조 모두 머신러닝 자체의 구현이나 성능과는 무관합니다.</p>
<ul>
<li>각각 장단점이 있겠네요.
<ul>
<li>1번의 경우 머신러닝 서비스 백엔드가 일종의 logger 역할도 하게 되므로 만약 감정 분류 결과가 부정확하거나 이슈가 발생한 경우 이를 다시 피드백으로 활용할 수 있을 것 같습니다.</li>
<li>2번의 경우 성능 측면에서 유리할 것 같습니다.</li>
</ul>
</li>
</ul>
<hr>
<h1>구현</h1>
<h3>HuggingFace KoBERT 기반으로 작성한 예시 코드</h3>
<ol>
<li>
<p>라이브러리 준비</p>
<pre><code># Transformers를 준비합니다.
!pip install transformers
!pip install sentencepiece

# KoBERT를 준비합니다.
# kobert_tokenizer 폴더를 다운로드 받습니다.
!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&amp;subdirectory=kobert_hf'
!pip install -U accelerate
</code></pre>
</li>
<li>
<p>데이터 준비</p>
<pre><code>import os
import pandas as pd
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim

from torch.utils.data import Dataset, DataLoader, random_split

# Google Colab과 연동해 Drive를 저장소로 사용합니다.
from google.colab import drive
drive.mount('/content/drive')

# 저장소에 있는 작업 디렉토리의 경로를 지정해줍니다. 아래의 경로는 예시입니다.
workdir = './drive/Colab Notebooks/test'

data = pd.read_csv(os.path.join(workdir, 'dataset.csv), encoding='cp949')
</code></pre>
</li>
<li>
<p>PyTorch 데이터셋 정의</p>
<pre><code># 텍스트를 입력하면 감정(label)을 출력합니다.

class SentenceDataset(Dataset):
    def __init__(self, sentences, labels=None):
        self.Sentences = sentences
        self.Labels = labels
    
    # 데이터의 개수를 지정해줍니다.
    def __len__(self):
        return len(self.sentences)
    
    # 지정된 개수만큼 데이터를 불러옵니다.
    def __getitem__(self.index):
        sentence = self.sentences[index]
        label = None
        if self.labels is not None:
            label = self.labels[index]
        return sentence, label
</code></pre>
</li>
<li>
<p>raw 데이터셋을 PyTorch 데이터셋으로 변환</p>
<pre><code># label을 지정해줍니다.

# data.Emotion.unique()의 출력값은 다음과 같습니다.
#     array(['분노', '행복', '불안', '당황', '슬픔', '중립', '혐오'], dtype=object)
labels = data.Emotion.unique()

# 데이터를 딕셔너리로 변환합니다. 각 감정(Emotion)에 0부터 6까지 인덱스(label)를 부여합니다.
label2id = {label:id for id, label in enumerate(labels)}
id2label = {id:label for label, id in label2id.items()}

data['label'] = data.Emotion.map(label2id)

# 전체 데이터의 일부(10%)만 학습에 사용해 빠르게 진행해보겠습니다.
data = data.sample(frac=.1)

# 이를 PyTorch용 학습 데이터셋으로 만들어줍니다.
dataset = SentenceDataset(data.Sentence.values, data.label.values)

# 학습 데이터셋과 별개로 validation 데이터셋을 준비합니다. 학습 데이터셋에다가 random_split을 적용해 9:1의 비율로 validation 데이터셋을 준비합니다.
num_data = len(dataset)
num_train = int(num_data*.9)
num_valid = num_data - num_train

train_dataset, valid_dataset = random_split(dataset, lengths=(num_train, num_valid))
</code></pre>
</li>
<li>
<p>모델과 tokenizer 불러오기</p>
<pre><code># 데이터를 KoBERT 모델에 투입해 분류, 태깅, 문장 생성 등을 수행하게 할 수 있습니다. 
from kobert_tokenizer import KoBERTTokenizer

# 여기서는 '분류'를 적용하겠습니다.
from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained('skt/kobert-base-v1', num_labels=len(labels), id2label=id2label, label2id=label2id)
tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')

# tokenizer 모델에 truncation=True 옵션을 넣어줘도 데이터는 원래의 길이 그대로 들어갑니다.
# 따라서 SequenceClassification 분류 모델이 최대로 입력받을 수 있는 길이에 맞게 (여기서는 512) tokenizer 모델의 데이터 길이를 먼저 제한해줄 필요가 있습니다.
# 토큰화 단계에서의 불필요한 메모리 과부하를 방지하기 위함입니다.
tokenizer.model_max_length = model.config.max_position_embeddings
</code></pre>
</li>
<li>
<p>Collator 정의</p>
<pre><code># PyTorch에서는 데이터의 크기와 인덱스를 입력해주면 데이터를 반환해주는 dataset이 있고,
# 이러한 dataset을 어떻게 shuffle 할지, sampling 할지, batch size는 무엇인지 등을 지정하고 수행하는 DataLoader가 있습니다.

# 만약 0부터 99까지의 숫자에 각각 문장 하나씩 짝을 지어 100개로 구성된 dataset이 있다고 해봅시다.
# batch_size=16, shuffle=True 옵션으로 DataLoader가 dataset을 불러오게 되면, 랜덤하게 섞이고 16개의 묶음으로 묶인 데이터가 준비됩니다.

# 이 데이터 묶음들을 모델에 실제로 적용해주기 위해 PyTorch Tensor 형식으로 변환을 하려고 합니다.
# 하지만 다차원 행렬인 Tensor에 기존 데이터를 넣어주기 위해서는 한 묶음 안에 들어있는 항목들의 길이가 모두 같아야 합니다.
# 지금까지 준비된 데이터는 토큰화도 되어있지 않고 문장 길이도 각각 다르기때문에, 가장 긴 문장의 길이를 기준으로, 그보다 짧은 길이의 문장에는 padding을 채워서 길이를 최대 길이로 같게 만들어줍니다.
# 바로 이 작업에 Collator를 활용합니다. tokenizer에 내장된 [PAD]라는 special token을 사용합니다. 

class Collator:
    def __init(self, tokenizer, pad='longest', truncation=True):
        self.tokenizer = tokenizer
        self.pad = pad
        self.truncation = truncation
    
    def __call__(self, batch):
        sentences = [b[0] for b in batch]
        labels = [b[1] for b in batch]
        tokenized = self.tokenizer(sentences, padding=self.pad truncation=self.truncation, return_tensors='pt')
        if labels[0] is not None:
            tokenized['labels'] = torch.tensor(labels)
        return tokenized
</code></pre>
</li>
<li>
<p>학습 파라미터 설정</p>
<pre><code>from transformers import TrainingArguments, Trainer

args = TrainingArguments(
    output_dir = os.path.join(workdir, 'outputs'),

    # 저장할 checkpoint 개수를 지정합니다.
    save_total_limit=1,

    # validation loss가 가장 낮은 checkpoint만 저장합니다.
    # overfitting이 일어나더라도 training loss가 아닌 validation loss를 기준으로 최적의 체크포인트만 저장하니 괜찮습니다.
    load_best_model_at_end=True,

    per_device_train_batch_size=32,
    per_device_eval_batch_size=64,
    num_train_epochs=1,
    evaluation_strategy='steps',
    save_strategy='steps',

    # 평가, 저장, 기록할 batch의 개수를 동일하게 지정해줍니다.
    eval_steps=50,
    save_steps=50,
    logging_steps=50,
)

trainer = Trainer(
    model,
    args=args,
    train_dataset = train_dataset,
    eval_dataset = valid_dataset,
    data_collator = Collator(tokenizer)
)

trainer.train()
</code></pre>
</li>
<li>
<p>학습된 모델 불러오기</p>
<pre><code>from transformers import pipeline

# 저장된 가장 최적의 체크포인트 경로를 지정해줍니다. 예: checkpoint-400 디렉토리
model = BartForSequenceClassification.from_pretrained(os.path.join(workdir, 'outputs', 'checkpoint-400'))

# Transformers의 pipeline 메소드를 사용하면 빠르게 추론이 가능합니다.
classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)

# 실제로 감정 분류기를 사용해봅시다. 아래의 예시 문장을 투입해봅시다.
classifier('오늘은 정말 힘든 하루였어')

# 출력값 예시: [{'label': '슬픔', 'score': 0.69395810298}]
</code></pre>
</li>
</ol>
<hr>
<h1>기타 조언</h1>
<ul>
<li>VM에서 배포를 하더라도 Flask 서버 코드는 가급적 파이썬 가상환경을 사용해서 독립적으로 유지보수하세요.</li>
<li>GPU로 학습시킨 모델이라고 하더라도 CPU를 사용해서 '서비스'가 가능합니다.</li>
<li>한국어 문장 분리에는 kss, Kiwi, KoalaNLP 등을 활용할 수 있습니다.</li>
</ul>
<hr>
<h1>참고자료</h1>
<ul>
<li><a href="https://github.com/Seolini/KoBERT_Korean_multi_classification/blob/main/KoBERT_%ED%95%9C%EA%B5%AD%EC%96%B4_7%EA%B0%9C%EA%B0%90%EC%A0%95_%EB%8B%A4%EC%A4%91%EB%B6%84%EB%A5%98.ipynb">KoBERT multi-class 7개 감정 분류</a></li>
<li><a href="https://github.com/BBARRY-Lee/Practice-NLP/blob/main/KoBERT%EB%A5%BC_%ED%99%9C%EC%9A%A9%ED%95%9C_%EA%B0%90%EC%A0%95%EB%B6%84%EB%A5%98_%EB%AA%A8%EB%8D%B8_%EA%B5%AC%ED%98%84.ipynb">KoBERT를 활용한 감정분류 모델 구현</a></li>
<li><a href="https://github.com/JH-lee95/Korean-Sentiments-Classification/blob/master/dataset_preprocessing.ipynb">감정 분류용 대화 데이터셋</a></li>
</ul>

    </div>
    
    <div class="mt-20 md:mt-32 lg:mt-32 xl:mt-32"></div>
</article>
        
    </main>
    <footer class="mt-20 px-10 py-8 bg-gray-200">
    <div class="max-w-5xl mx-auto text-gray-700 text-center">
        © 2022 <a href="/" class="font-medium" target="_blank" rel="noopener">devfreedom.github.io</a> by 
        <a href="https://devfreedom.github.io" target="_blank" rel="noopener">devfreedom</a> 
    </div>
</footer>
    <script src="/assets/js/bundle.js"></script>
</body>

</html>