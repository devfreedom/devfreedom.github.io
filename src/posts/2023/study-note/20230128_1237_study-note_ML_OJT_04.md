---
title: "[Study Note] Coursework: The Comprehensive Practice in Machine Learning - Part 4"
date: 2023-01-28T12:37
thumb: "artificial-intelligence.jpg"
tags: 
    - ❮Study Note❯
    - machine learning
---

See you again in next CompSci semester, with "Artificial Intelligence 101" which is even more dreading...

### Genetic algorithm
- "Survival of the fittest" principle
    - Genetic algorithm is introduced by John Holland and Christopher Langton, based on John Von Neumann's theoretic basis
    - A metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA)
    - Generates high-quality solutions to optimization and search problems by relying on biologically inspired operators
- Operators
    - Crossover
        - Too many crossovers don't guarantee "better" solutions, just "many" solutions
    - Mutation
        - Diversifying the solutions so that local optima could be avoided
        - Single-point mutation
            - Generating a random variable for each bit in a sequence, this random variable tells whether or not a particular bit will be flipped
                - Intentional data corruption
    - Selection
- Workflow
    - Requirements
        - A genetic representation of the solution domain
        - A fitness function to evaluate the solution domain
    - Step 1. Define problem/solution/array-of-bits
    - Step 2. Define chromosomes of the solution
    - Step 3. Build a fitness function "f(x)"
    - Step 4. Genetic manipulation (crossover, mutation, etc.)
    - Step 5. Perform chromosome selection
    - Step 6. Repeat generational process until a termination condition has been reached
    - Fitness function evaluation

### Association rule learning
- Finding a rule among elements within a given set of data
    - e.g. Analyzing customers' "saved for later" items
- Frequent itemset
    - A set of items that is frequent in a subset
- Infrequent itemset
    - A set of items that is not frequent in a subset
- X → Y
    - The representation of an association rule
        - Y exists if X exists
        - e.g. 
            - X = ["Boots", "Sneakers", "Oxford shoes"]
            - Y = ["Insole", "Polisher", "Shoelaces"]
    - s(X⇒Y)
        - = support = n(X∪Y) / N = P(X∪Y)
        - N = universal set = transaction
    - c(X⇒Y)
        - = confidence = n(X∪Y) / n(X) = P(Y|X)
    - Lift = P(Y|X) / P(Y) = P(X∪Y) / P(X)P(Y) = c(X⇒Y) / P(Y) 
        - Lift(A, B) = c(A→B) / s(B)
        - A lift value greater than 1 indicates that the rule body and the rule head appear more often together than expected
            - The occurrence of the rule body has a positive effect on the occurrence of the rule head.
            - e.g. Buying a shoe polisher after buying boots is an "improvement" over just buying a shoe polisher only
        - If A and B are independent, then the lift value would be 1 which means "no improvement"
        - A lift smaller than 1 indicates negative effect
- Apriori algorithm
    - An algorithm for frequent item set mining and association rule learning over relational databases

### Logistic regression analysis
- Sigmoid function
    - Sigmoid(x) = 1 / (1+e<sup>-x</sup>)
    - Sigmoid function converts a certain real number into 0~1 which is the range of probabilities
- Generic regression analysis
    - y = w<sup>T</sup>x + b
- Logistic regression analysis
    - sigmoid(x) = σ(y) = σ(w<sup>T</sup>x+b)
    - Returns the output value ranging from 0 to 1
    - LRA is about predicting the probability of either A or B
- P(Y=1|x)
    - The probability if Y is true when x exists
    - The probability of a 'positive' correlation
    - = σ(w<sup>T</sup>x+b)
- P(Y=0|x)
    - The probability if Y is false when x exists
    - The probability of a 'negative' correlation
    - 1 - P(Y=1|x) = 1 / { σ(w<sup>T</sup>x+b) }
- P(Y=1|x) / 1 - P(Y=1|x)
    - = odds ratio
    - = e<sup>w<sup>t</sup>x+b</sup> = sigmoid function
- ln(odds ratio) = w<sup>T</sup>x + b = generic regression

### Time series analysis
- Used when independent variables changes over time
- Auto-correlation
    - The relation where the past of a variable affects the future of the variable
- x<sub>t</sub> = T<sub>t</sub> + S<sub>t</sub> + a<sub>t</sub>
    - Time series model
    - T = Trend
        - The increasing or decreasing tendency of the data, without frequency in general
    - S = Seasonality
    - a = Noise
    - a<sub>t</sub> ~ N(0, σ)
- Normality
    - Requirements
        - Average is flattened for any point in time "t"
            - E(Z<sub>t</sub>) = µ
        - Variance "Var(Z<sub>t</sub>)" is not dependent on the point in time "t"
        - When there are two points in time "t" and "s", the covariance "Cov(Z<sub>t</sub>, Z<sub>s</sub>)" is dependent on the lag (time difference) "t-s", but not "t" or "s"
    - To achieve "normality" from "anomaly with trend", differentiation is used
    - Normality needs to be achieved to apply ARIMA model
- - Auto-correlation function
    - A function that measures the relation between observed values of the lag "k"
    - ρ<sub>k</sub> = Cor(y<sub>t</sub>, y<sub>t+k</sub>) / Var(y<sub>t</sub>)
- Partial auto-correlation function (PACF)
    - A function that measures the relation of y<sub>t</sub> and y<sub>t-k</sub>, except the lag "k"

### Machine learning model evaluation
- Metrics
    - Confusion matrix
        - [ Predict = Y, Actual = Y ] = True positive (TP)
        - [ Predict = Y, Actual = N ] = False positive (FP)
        - [ Predict = N, Actual = Y ] = False negative (FN)
        - [ Predict = N, Actual = N ] = True negative (TN)
        - Accuracy = (TP + TN) / (TP + FP + FN + TN)
        - Precision = TP / (TP + FP)
        - Sensitivity = Recall = TP rate = TP / (TP + FN)
        - Specificity = TN / (FP + TN)
        - FN rate = FP / (FP + TN)
    - Receiver operating characteristic (ROC) chart
        - X-axis: FP rate
        - Y-axis: TP rate
        - If the ROC curve is closer to the top left corner, the model is better
            - Area under curve (AUC) needs to be greater than 0.5
    - Hold-out
        - Seperate testing data from the original data, and train the model using the rest (training data)
        - Drawbacks
            - Test data is not diverse enough
            - Model might be biased            
    - K-fold cross validation
        - Split the data into N number of blocks, create N number of models, and then perform N number of tests
        - Overcomes the drawbacks of hold-out method
        - e.g. [Data 1, Data 2, Data 3]
            - Test Data 1 by using Model A, calculate the performance A<sub>1</sub>
            - Test Data 2 by using Model B, calculate the performance B<sub>2</sub>
            - Test Data 1 by using Model C, calculate the performance C<sub>3</sub>
