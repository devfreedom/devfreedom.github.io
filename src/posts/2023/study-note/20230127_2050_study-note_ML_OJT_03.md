---
title: "[Study Note] Coursework: The Comprehensive Practice in Machine Learning - Part 3"
date: 2023-01-27T20:50
thumb: "artificial-intelligence.jpg"
tags: 
    - ❮Study Note❯
    - machine learning
---

### Word2Vec
- Natural language processing technique that overcomes the limitation of bag-of-words model
    - BoW model doesn't reflect distance and relations between words
- Represents each distinct word with a particular list of numbers called a vector
- Produces word embeddings in a specified size
- e.g. ["The quick brown fox jumps over the lazy dog."]
    - The
        - ["the", "quick"]
        - ["the", "brown"]
    - Quick
        - ["quick", "the"]
        - ["quick", "brown"]
        - ["quick", "fox"]
    - Brown
        - ["brown", "the"]
        - ["brown", "quick"]
        - ["brown", "fox"]
        - ["brown", "jumps"]
    - ...
    - Skip-gram
        - A generalization of n-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are skipped over
- Input layer → hidden layer → output layer
    - = x<sub>1</sub>, x<sub>2</sub>, ... x<sub>V</sub> → h<sub>1</sub>, h<sub>2</sub>, ... h<sub>N</sub> → y<sub>1</sub>, y<sub>2</sub>, ... y<sub>V</sub>
    - = V-dim → N-dim → C×V-dim
    - W<sub>(V×N)</sub>={w<sub>ki</sub>} → W'<sub>(N×V)</sub>={w'<sub>ij</sub>}
        - The input or the context word is a one-hot-encoded vector of size V
            - V = dictionary size
        - The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values
            - N = user-defined value
        - W<sub>(V×N)</sub> = hidden layer, weight matrix
        - W'<sub>(N×V)</sub> = word vector, lookup table

### Topic modelling
- Statistical deduction algorithm for finding a hidden topic structure
- Words → topics → documents → text corpus
    - = Three-stage hierarchical Bayesian model
    - A topic has a set of words
    - A document is made of multiple words
    - A document has one or more topics
    - Words are a variable that indicates a specific topic
    - e.g.
        - words = ["function", "variable", "operator", "iterator", "method", "class", "encapsulation", "abstraction"]
        - → topic = "programming"
- β<sub>ik</sub>
    - = the probability of the word "W<sub>i</sub>" to reflect a certain possible topic "k"        
        - The probability of the topic "Z<sub>k</sub>" to generate a certain set of words is,
            - Z<sub>k</sub> = Σ(β<sub>ik</sub>×W<sub>i</sub>)
                - We can calculate the probability of a specific document to be generated as well
    - e.g. 
        - Document 1 = ["operator", "method", "abstraction"]
        - Document 2 = ["function", "operator", "method"]
        - Document 3 = ["operator", "probability", "sum"]
        - Topic 1 = programming = ["function", "operator", "method", "abstraction"]
        - Topic 2 = mathematics = ["function", "operator", "probability", "sum"]
        - Topics have different weights in the documents
        - What if we don't know what the topic is?
            - We have to deduct the topics from the documents
- Latent dirichlet allocation (LDA)
    - A generative statistical model that explains a set of observations through unobserved groups, and each group explains why some parts of the data are similar
    - Premise
        - There are "k" numbers of topics
        - There is an "i"th word "W<sub>ik</sub>
        - There is a topic vector "Θ" in a certain document
        - β<sub>ik</sub> = The weight/probability of the topic to generate a certain word
        - Z<sub>ik</sub> = The topic "k" of the "i"th word
        - W<sub>ik</sub> = The word that indicates the topic "k" of the "i"th word
    - Extract the topic "Z" from the topics distribution Θ
    - Generate the word "w" with β
    - α → corpus = M = [ → Θ → document = N [ → z → w ] ]

### Random forest & AdaBoost
- **Ensemble learning**
    - The use of multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone
- **Bootstrap aggregating (bagging)**
    - Using bootstrapped training data sets for the same algorithm
    - A bootstrapped set is created by selecting from original training data set with replacement
        - A bootstrap set may contain a given example zero, one, or multiple times.
    - Given a training set X = x1, ..., xn with responses Y = y1, ..., yn, bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to such samples
    - e.g. 
        - Sample data 1 → (training) → weak model A
        - Sample data 2 → (training) → weak model B
        - Sample data 3 → (training) → weak model C
        - Weak model A, B, C → (majority voting) → strong model
- **Boosting**
    - An ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones
    - Iteratively learning weak classifiers with respect to a distribution and adding them to a final strong classifier
        - When they are added, they are weighted in a way that is related to the weak learners' accuracy
        - After a weak learner is added, the data weights are readjusted, known as "re-weighting"
    - Taking errors into account by weighting lower-accuracy models
    - e.g. 
        - Weaker sample data 1 → (weighted re-sampling, training) → weak model A
        - Sample data 2 → (training) → weak model B
        - Sample data 3 → (training) → weak model C
        - Weak model A, B, C → (weighted voting) → strong model
    - AdaBoost
        - Statistical classification meta-algorithm
        - Subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers
- **Random forest**
    - An ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time
    - Using "decision tree" method for bootstrap aggregation
    - Used for classifier, regression analysis, cluster analysis
    - Less impact from noise
    - Possible overfitting issue if the sample data is not big enough

### Social network analysis (SNA)
- The process of investigating social structures through the use of networks and graph theory
- Network structure
    - Node
        - Attributes: color, shape, size, etc.
    - Link/edge
        - Attributes: thickness (connection strength)
        - Directional interaction 
        - Non-directional interaction
    - Degree
        - The number of directional links that a certain node has
        - → indegree → node → outdegree → 
        - ← outdegree ← node ← indegree ←
    - Density
        - = the sum of all node's indegree and outdegree count ÷ the number of cases of all possible links
        - more links, higher density
    - Reachability
        - The possibility if node A can reach to node B, either directly or indirectly
        - Reachability is either 0 (impossible) or 1 (possible)
    - Component
        - A group of nodes that are internally reachable
    - Centrality
        - Closeness centrality
        - Betweenness centrality
        - Eigenvector centrality

### Combining machine learning models
- Practice 1. Naive Bayes + TF-IDF
- Practice 2. Random forest + TF-IDF
- Practice 3. Random forest + Word2Vec + Document2Vec