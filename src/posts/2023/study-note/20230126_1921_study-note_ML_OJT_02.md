---
title: "[Study Note] Coursework: The Comprehensive Practice in Machine Learning - Part 2"
date: 2023-01-26T19:21
thumb: "artificial-intelligence.jpg"
tags: 
    - ❮Study Note❯
    - machine learning
---

### Naive bayes
- **Bayes' theorem**
    - The probability if Event A and Event B are independent = P(A) × P(B)
    - The probability if Event B happens under Event A's condition = P(B|A) 
    - The probability of Event A and Event B in a row = P(A) × P(B|A)
        - P(A) = prior probability
        - P(B|A) = posterior probability
        - P(A) × P(B|A) = P(A⋂B)
    - The probability of Event B and Event A in a row = P(B) × P(A|B) = P(A⋂B)
    - P(A) × P(B|A) = P(B) × P(A|B) = P(A⋂B)
    - P(A|B) = { P(B|A) × P(A) } / P(B) = P(A⋂B) / P(B)
- Bayes' theorem use case in machine learning
    - e.g. Calculate the probability if a certain keyword should be classified as 'spam' or not
        - If there is only one keyword to classify
            - P(spam|'extended car warranty') = P('extended car warranty'|spam) × P(spam) / P('extended car warranty')
        - If there are multiple keywords to classify
            - Premise
                - Events happen concurrently
                - Extinguish uncalculable probabilities by assuming the probabilities are the same
                    - Simplify the usage of Bayes' theorem, which is not axiomatized, and not worrying too much about potential paradoxes, hence "Naive Bayes"
                - Array X = the list of keywords
            - P(spam|X) = P(spam) × P(X<sub>1</sub>|spam) × P(X<sub>2</sub>|spam) × P(X<sub>3</sub>|spam) ...
                - P(not spam|X) = P(not spam) × P(X<sub>1</sub>|not spam) × P(X<sub>2</sub>|not spam) × P(X<sub>3</sub>|not spam) ...
        - If there are multiple keywords to classify into multiple groups
            - Use multiclass option
    - Suitable for text mining to predict and classify the classes of given text data

### Support vector machine (SVM)
- About finding a proper dividing line (= hyperplane) on a scatterplot, which seperates/divides/classifies data of different classes 
    - Support vector = the parallel-translated hyperplane that touches the nearest data point
    - Margin = the distance between the original hyperplane and support vectors
    - SVM is about finding the support vector with a maximum margin
- **d = ω<sup>T</sup> × x + ω<sub>0</sub>**
    - d = hyperplane, the function we need to find
    - If the data/cluster is non-linear, then it needs to be converted/mapped to linear
        - e.g. If there is a circular cluster of data
            - x² + y² = r² → y = √(r² - x²)
        - Transformation is abstracted in Φ(x), which means the function to map linear/non-linear data
            - d = ω<sup>T</sup> × Φ(x) + ω<sub>0</sub>
            - Ironically this makes defining a hyperplane more difficult, a.k.a "the curse of dimensionality"
        - Representer theorem defines weighted ω as W = Σα<sub>i</sub>Φ(x<sub>i</sub>)
            - weighted d' = Σ{α<sub>i</sub>×Φ(x<sub>i</sub>)×Φ(x<sub>i</sub>)} + ω<sub>0</sub>
                - Φ(x<sub>i</sub>)×Φ(x<sub>i</sub>) = K(x<sub>i</sub>, x) = kernel function
                    - We can define a hyperplane if we can figure out a kernel function
                    - There are pre-defined kernel functions for certain data patterns, which we can utilize to build a SVM model
- **Common kernel function types**
    - Linear
    - Non-linear
    - Polynomial
    - Gaussian radial basis function (RBF)
    - Laplacian radial basis function (RBF)
    - Sigmoid 
- Performant for non-linear and high-dimensional data
- Supports multiclass
- Good for complex real-life data model
- Difficult to grasp its mathematical algorithm
- Resolves decision tree's drawback
    - Decision tree splits data in a linear way
        - which is not suitable for non-linear datasets

### Term frequency - inverse document frequency (TF-IDF)
- Text is made of sentences
    - Sentence is made of words
        - Morpheme is the smallest element to use for text analysis
- Morpheme analysis libraries are not one-size-fits-all
    - Choose a library based on the needs of text mining and the context of the data
    - Diverse testings are needed
- e.g. Dataset A
    - ["The quick brown fox jumps over the lazy dog", "Lorem ipsum dolor sit amet, consectetur adipiscing elit.", "Teach Your Dog How To Jump Over Things", "The Fox (What Does the Fox Say?)", "Brown University is a private Ivy League research university in Providence, Rhode Island." ]
    - How to distinguish the importance of words?
        - Pre-processing
            - Remove the words that are not meaningful varaibles
                - e.g. "the", "over", "how", "what", "is", "a", "in"
                - Articles, interrogative pronouns, etc.
                - These are called "stop words"
        - Term frequency score (TF score)
            - TF score represents the frequency of certain words
            - The ratio of the frequency of certain words to the frequency of all words
            - TF<sub>ij</sub> = F<sub>ij</sub> / ΣF<sub>kj</sub>
                - F = frequency
                - j = document
                - i = certain word
                - k = all words
            - Issue
                - Bag-of-words model
                    - A text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity
                - "Are words that appear more frequently and prevalent the ones that are more important?" 
                    - "Not always."
                        - Because TF score is roughly proportional to the length of the document
                        - Not an accurate representation of "importance", just "prevalency"
        - Inverse document frequency (IDF)
            - "The words that are included in important documents are the ones that are more important."
            - The inverse of "the proportion of certain documents that has certain words from all documents"
                - Inverse because it means that those certain words are more distant from common words, which emphasizes its importance
            - IDF<sub>i</sub> = log |D| / |(D<sub>ji</sub> ∈ D<sub>j</sub>)|
                - |D| = the number of all documents
                - |(D<sub>ji</sub> ∈ D<sub>j</sub>)| = the number of documents that have certain words
        - TF-IDF score
            - To properly calculate the importance of certain words, we can use TF-IDF score which implies both "frequency" and "quotation" of the words
            - TFIDF<sub>ij</sub> = TF<sub>ij</sub> × IDF<sub>ij</sub>

### Principal component analysis (PCA) & DBSCAN
- Data is made of different classes/attributes
    - Data is easier to analyze and comprehend when it's converted from high-dimension to low-dimension
        - e.g. 3-axis 3D graph → 2-axis scatterplot with a corresponding linear graph for the reduced axis
        - a.k.a. dimensionality reduction
        - To avoid "the curse of dimensionality"
- **Principal component analysis (PCA)**
    - Building a new axis that reduces dimension while keeping the original data as intact as possible
        - The new axis should represent the most previous changes
    - X<sup>T</sup> × X = V × D × V<sup>T</sup> , X = U × V<sup>T</sup>
        - X = original data
        - N = number of data
        - P = original dimension
        - U = scores matrix = N × K matrix
        - V = loading matrix = orthogonal matrix = P × K matrix
        - D = diagonal matrix
    - Finding the right K that minimizes information loss
- **Density-based spatial clustering of applications with noise (DBSCAN)**
    - K-means clustering has some issue with stretched, narrow, long-shaped data because it's suitable for circle-shaped data
        - DBSCAN can be an alternative for such a case
    - PCA-DBSCAN workflow
        - Step 1. Data compression
        - Step 2. Calculate kNN distance function
        - Step 3. Perform cluster analysis
    - Neighbor vector = A set of data vector (dot objects) which is included in a circle that centers a certain data vector and has "Ɛ" radius
    - Core vector = A data vector that has "n" number of neighbor vectors
        - n = threshold/cutoff value to determine if it's a core vector or not
    - The relation between core vector "p" and its neighbor vector "q" is represented as "p → q"
        - = q is directly accessible to p
        - If "p → p1 → p2 → ... → q", then "p ⇒ q"
            - = q is accessible to p
        - If "r ⇒ p", "r ⇒ q", then "p ⇔ q"
            - = p and q are linked
    - Cluster = a collection of all data vectors which is accessible to one core vector "p"
    - Noise = any data that don't belong to any clusters

### Neural network
- Machine learning methodology that mimics biological cerebral nerves
    - Computer learns to perform some task by analyzing training examples
    - Perceptron
        - Introduced by Frank Rosenblatt
        - An algorithm for supervised learning of binary classifiers
        - e.g. The first ever perceptron concept 
            - There are three input nodes: x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>
                - x<sub>1</sub> is carried by weight w<sub>1</sub>=0.1
                - x<sub>2</sub> is carried by weight w<sub>2</sub>=0.2
                - x<sub>3</sub> is carried by weight w<sub>3</sub>=0.3
            - There is a bias: b = -0.4
                - Bias value allows you to shift the activation function curve up or down
            - Weighted sum of three inputs and bias = S = 0.1x<sub>1</sub> + 0.2x<sub>2</sub> + 0.3x<sub>3</sub> - 0.4
            - Apply the weighted sum to the activation function 
                - Signum/sign function
                    - One of elementary neural activation functions to activate each node in the neural network by scaling the input values to range between -1 to 1
                        - y = sign(S)
                    - If the weighted sum "S" is equal to or greater than the threshold "t", return the output "1"
                        - Input is accepted and passed as an output
                    - If the weighted sum "S" is lesser than the threshold "t", return the output "-1"
                        - Input is dismissed/ignored
    - Multi-layer perceptron
        - Introduced by Marvin Minsky
        - Input layer → hidden layer → output layer
            - Input layer → hidden layer
                - Output = O<sub>h</sub> = f(W<sub>x</sub>+b)
            - Hidden layer → output layer
                - Output = O<sub>o</sub> = f(WO<sub>h</sub>+b)
        - Gradient descent
            - The process of using gradients to find the minimum value of the cost function
            - Backpropagation
                - Calculating those gradients by moving in a backward direction in the neural network
    - Types of activation function
        - Signum/sign function
            - Output is either -1 or 1
        - Step function
            - Output is either 0 or 1
        - Linear function
        - Sigmoid function
        - Hyperbolic tangent function
        - Softmax function
        - ReLU
        - Leaky ReLU
        - Swish function