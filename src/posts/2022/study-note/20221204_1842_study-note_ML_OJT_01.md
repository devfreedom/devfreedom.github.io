---
title: "[Study Note] Coursework: The Comprehensive Practice in Machine Learning - Part 1"
date: 2022-12-04T18:42
thumb: "artificial-intelligence.jpg"
tags: 
    - ❮Study Note❯
    - machine learning
--- 

The place where I worked recently had provided online IT courses for employees, so I had decided to dip my toe in machine learning to see and learn how it actually works. I had to expect a rough start because I didn't have much relevant background knowledge (especially mathematics) prior to this course and machine learning itself is quite an advanced study. And yes, it turned out to be pretty rough. Even though this course wasn't really beginner-friendly unlike what it said, I could at least understand the underlying mechanisms of machine learning techniques. Skimming on the theoretical background was my primary goal so it was okay for that.

The following series of articles are a summary of what I've learned from this course. This course came with practice sessions where you could get hands-on experience in machine learning, but I didn't have enough free time to code those things at that time. I'm planning to get back to the material and try it by myself in the near future.


### Use cases of machine learning 
- Regression
    - Predict the future outcome of the target attribute, based on the input data
    - Heavy use of statistics
        - Regressional analysis, multivariate statistics, time series analysis, etc.
    - e.g. Stock market price, FOREX rate, risk analysis, CRM, ad bidding, etc.
- Classification
    - Predict the class or decide the rank of individual data, based on the input data
    - e.g. Fraud detection system, spam mail filtering, drug efficacy verification
- Suggestion
    - Recommend personalized options and alternatives
    - e.g. Automated Spotify playlist, LinkedIn's 'people you may know' feature
- Alteration
    - Strengthen the input data by interpolating or estimating missing values
    - e.g. Repairing corrupt data, improving the quality of the census data

### Machine learning methodology
- Supervised learning
    - Used when the training data has a target variable inside
- Unsupervised learning
    - Used when the training data doesn't have a target variable in it
    - Extract data patterns without making function models
        - To discover a hidden structure inside the data
- Reinforcement learning
    - Data interacts with a machine learning environment to find the answer by itself
    - Training while generating the target variable
    - Gamification = reward and penalize based on how approximate it is to the answer
    - e.g. Object detection    

### Machine learning workflow
- Step 1. Feed the existing data
- Step 2. Exploratory data analysis (EDA) 
    - Data feature extraction, seperation of train/validation/test data
- Step 3. Modelling
    - Establish a model → evaluate the model → optimize the model
- Step 4. Get the new predicted data
    - → Feed the new data into the model for model improvement
        - Continuous feedback and evaluation is needed

### Machine learning workflow pseudocode
```
# Data pre-processing
data = load_data("raw_data.csv")
data_final = process_data(data)

# Cleanse missing and biased data
detect_outlier(data)
munge_data(data)
wrangle_data(data)

# Seperate training data from testing data
split_data(data_final)

# Evaluate a model by comparing actual data to predicted data
model = build_model(training_data)
predicted_data = model.predict(testing_data)
model_accuracy = compare(predicted_data, testing_data.target)
```

### Decision tree
- The sorting that reduces entropy the most is the best
    - "How to classify things in the most certain way possible?"
    - Probability = the ratio of specific data to the whole
    - Entropy = a (measurable) state of disorder, uncertainty and randomness
        - Information entropy = −∑xP(x)log<sub>2</sub>P(x)
            - log<sub>2</sub> = number of bits needed to express the information
            - i = variety = "how many baskets do we need to group these items into?"
            - p(x<sub>i</sub>) = the possibility of picking the specific group of items from the whole
            - If the probability is 1 then the entropy is 0
            - e.g. 3 blue pencils, 2 red pencils in a box
                - Information of blue pencils = −log<sub>2</sub>(3/5)
                - Information of red pencils = −log<sub>2</sub>(2/5)
                - Entropy = −(3/5)log<sub>2</sub>(3/5)−(2/5)log<sub>2</sub>(2/5) = 0.97
            - The average value of all necessary bits
            - The average expected value of the information
        - The more information it has, the more storage it needs, the bigger the entropy is
        - Information gain = delta of entropy
            - Entropy(T) − Entropy(A)
            - Entropy before split minus entropy after split
- Root node → 1st decision nodes → 2nd decision nodes → ... → leaf/terminal nodes
    - Leaf nodes = target attributes
    - Data gets splitted at each iteration
    - Repeat until the entropy becomes close to 0
    - Making a good decision tree = maximizing information gains
        - Decision nodes should be placed in ascending order of entropy
        - Most information gain → least information gain
    - Data distribution changes as the split iterates
        - Re-calculate the entropy again to find the attribute that has the most information gain
- **Notable decision tree algorithms**
    - CART
    - C4.5
    - C5.0
    - Commonality of decision tree algorithms
        - Uses entropy
        - Uses Gini Index
        - Related to how to split and classify data
        - Uses the most influential attribute
- **Entropy alternatives**
    - Entropy calculation is relatively slow due to logarithmic function
    - Gini Index = 1−∑pi²
    - Chi-square = x² = ∑²∑²(n<sub>ij</sub>−E<sub>ij</sub>)/(E<sub>ij</sub>)
        - n<sub>ij</sub> = observed value
        - E<sub>ij</sub> = expected value
        - Unlike entropy which splits data into the most certain way, Chi-square splits data into the least relevant way

### Regression analysis
- **Least squares approximation**
    - "What linear equation on a scatterplot would accurately represent the data, with minimum errors?"
    - Error = model-based linear equation − actual data
        - Expressed as cost/loss/error function
            - Cost function = ∑<sub>i</sub> = (y-y<sub>i</sub>)² = ∑e² = sum of squared deviation
        - The distance between linear equation and actual data
            - Absolute value distance = |f(x)−y|
            - Minkowski distance
            - Squared distance = (f(x)−y)²
        - Generalized Linear Model
- **Essential regression analysis**
    - Finding the graph that minimizes the cost function
        - = finding the model that moves towards the minimum cost function, by using 'least squares approximation'
    - y=f(x)
        - x = actual value = independent variable
        - y = predicted value = dependent/target variable
    - Simple regression = when there is one single independent variable
    - Multiple regression = when there are more than one independent variables
    - Uses supervised learning to predict the data
    - In general, social science uses linear regression model, while natural science uses non-linear regression model
- **Regression analysis evaluation**
    - Coefficient of determination = R-squared(R²)
        - "How well does the model represent the data?"
        - R² is between 0 and 1 (the higher the better)
        - The model with R² above 0.6 in social science, above 0.9 in natural science, is considered as acceptable
    - F-statistic(= F-Value) and P-value
        - Statistical evaluation of the 'reliability' of the regression model
        - F-statistic = the higher the better
        - P-value should be lower than 0.05 (statistically significant) or 0.01 (highly significant)

### Clustering analysis
- **Distance measure**
    - Variables = x = { x<sub>1</sub>, x<sub>2</sub>, ... x<sub>k</sub> }
    - Distance matrix = D = { D<sub>ij</sub> } 
    - "What is distance? How to define the distance between data?"
        - The distance needs to be defined as formula
    - Common basic distance measures
        - Euclidean distance
        - Manhattan distance
        - Minkowski distance
- **Clustering methods**
    - Single-linkage clustering
        - Choose the minimum value among distances between clusters
    - Complete-linkage clustering
        - Choose the maximum value among distances between clusters
    - Average-linkage clustering
        - Calculate the average value among distances between clusters
    - Centroid-linkage clustering
        - Calculate the distance between average vectors of each cluster
        - Uses weighted average based on each cluster's size
        - x̅ = (n<sub>1</sub>x̅<sub>1</sub> + n<sub>1</sub>x̅<sub>2</sub>) / (n<sub>1</sub> + n<sub>2</sub>)
    - Ward-linkage clustering
        - Error Sum of Squares(ESS) distance
            - The linkage function specifying the distance between two clusters is computed as the increase in the "error sum of squares" (ESS) after fusing two clusters into a single cluster
            - Ward's Method seeks to choose the successive clustering steps so as to minimize the increase in ESS at each step
    - All the methods above performs clustering by fusing the nearest (= minimum distance) data one by one, until there is one single cluster left
- **Types of clustering analysis**
    - Dendrogram
        - Calculate all data points by using distance matrix
        - Hierarchical clustering
        - Requires much memory space
        - Difficult to use when the data is big enough
        - Useful to see the overall cluster pattern
    - K-means clustering
        - Calculate average vector among data
        - Determine the cluster by repeating the calculation of cluster's average vector until it doesn't change anymore
        - Non-hierarchical clustering
        - Not suitable for non-circular clusters
        - K-means workflow
            - e.g. 2 clusters
                - Step 1. Calculate all distances between the center of the cluster and all data points
                - Step 2. Check the nearest cluster core for each data points
                - Step 3. Assign a cluster to each data points
                - Step 4. Calculate average vector among data points in the same cluster
                - Step 5. Calculate all distances between the new center of the cluster and all data points, repeat until there is no change in the center position
                    - The movement of the cluster center will decrease after each iteration
        - How to determine the numbers of clusters to define?
            - There is no algorithm or rule to accurately determine the number
            - We can only assume the most efficient number of clusters through visualization or Exploratory Data Analysis (EDA)
            - Too many clusters are not efficient
    - DBSCAN
        - Bind data into clusters by density
        - Group by higher density 
        - Non-hierarchical clustering
        - Can detect noises automatically
        - Catches elliptic clusters as well

### K-Nearest Neighbor
- Examine 'K' numbers of data nearby and predict the 'most voted' attribute
    - K is subjective, not determined, therefore multiple tests are needed
    - Calculate first, and then classify 
        - = Case-based learning
        - = Memory-based learning
        - = Supervised learning
- e.g. Red pencils and blue pencils in a box
    - Premise
        - There is one type of attribute = color
        - There are two levels of attribute = red / blue
        - There are pencils that we don't know its color
        - Let's assume that there are 5 neighbors in one cluster, K = 5
    - Group 5 pencils as a cluster and check the color
        - Cluster A has 3 red pencils, 2 blue pencils, and one pencil of unknown color
            - Since there are more red pencils than blue pencils, let's assume the unknown color to be 'red'
        - Cluster B has 2 red pencils, 3 blue pencils, and one pencil of unknown color
            - Since there are more blue pencils than red pencils, let's assume the unknown color to be 'blue'
    - Weighted KNN
        - Previously we ignored the distance between pencils
        - y = argmax<sub>v</sub>∑<sub>D<sub>x</sub></sub>I(v=y<sub>i</sub>)
            - argmax = find the 'v' value that gives the maximum solution
                - 'v' is the target attribute
- **Possible issues**
    - Deviation
        - Different dataset has different deviation
        - Data normalization is recommended
            - Workaround 1 : Convert all data values into 0(min)~1(max)
            - Workaround 2 : Convert all data values into Z-score
    - Accuracy and reliability depends on K value
        - If K is too small, overfitting happens
        - If K is too big, underfitting happens
