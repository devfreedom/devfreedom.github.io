<!DOCTYPE html>
<html>
    
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>[필기] 컴퓨터공학과 &lt;인공지능&gt; 과목 - 후반부 | devfreedom.github.io</title>
    <meta name="title" content="[필기] 컴퓨터공학과 &lt;인공지능&gt; 과목 - 후반부 | devfreedom.github.io">
    <meta name="description" content="devfreedom&#39;s personal blog">
    <meta name="keywords" content="❮필기❯,인공지능,머신러닝,파이썬,eleventy,template,simple,clean">
    <meta name="author" content="devfreedom">
    <meta name="robots" content="index, follow">
    <link rel="canonical" href="https://devfreedom.github.io/20230423_1106_study-note_compsci_AI_02/">
    <link rel="shortcut icon" type="image/png" href="/assets/img/favicon.svg">
    <link rel="apple-touch-icon" href="/assets/img/apple-touch-icon.png">
    <link rel="dns-prefetch" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="dns-prefetch" href="https://fonts.gstatic.com">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap">
    <link rel="stylesheet" href="/assets/css/main.css">
    
</head>	

<body class="flex flex-col h-screen bg-white text-gray-800 break-words">
    <header id="header" class="header-shadow bg-black px-6 py-5 z-50 fixed w-full top-0 transition-all transform ease-in-out duration-500">
    <div class="max-w-5xl mx-auto flex items-center flex-wrap justify-between">
        <div class="sm:mr-8">
            <a class="flex items-center" href="/">                              
                <span class="text-lg text-white font-semibold self-center">devfreedom.github.io</span>
            </a>
        </div>
        <nav id="menu" class="order-last md:order-none items-center flex-grow w-full md:w-auto md:flex hidden mt-2 md:mt-0">
            
            <a href="/tags" class="block mt-4 md:inline-block md:mt-0 font-medium text-gray-500 hover:text-white text-base mr-7">Categories</a>
            
            <a href="https://github.com/devfreedom" target="_blank" rel="noopener" class="block mt-4 md:inline-block md:mt-0 font-medium text-gray-500 hover:text-white text-base mr-7">GitHub</a>
            
            <a href="/about" class="block mt-4 md:inline-block md:mt-0 font-medium text-gray-500 hover:text-white text-base mr-7">About</a>
            
        </nav>
        <form id="search" action="/search" class="order-last sm:order-none flex-auto w-32 items-center justify-end hidden sm:block mt-6 sm:mt-0">
            <label class="visually-hidden" for="header-searchbox">Search here ...</label>
            <input type="text" id="header-searchbox" name="q" placeholder="Search..." class="w-full sm:max-w-xs bg-gray-200 border border-transparent float-right focus:bg-white focus:border-gray-300 focus:outline-none h-8 p-4 placeholder-gray-700 rounded text-gray-700 text-sm">
        </form>
        <div id="menu-toggle" class="flex items-center md:hidden text-gray-700 hover:text-teal-600 cursor-pointer sm:ml-6">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
    <line x1="3" y1="12" x2="21" y2="12"></line>
    <line x1="3" y1="6" x2="21" y2="6"></line>
    <line x1="3" y1="18" x2="21" y2="18"></line>
</svg>
        </div>
    </div>
</header>
    <main class="mx-7 lg:mx-6 mt-32 flex-grow">
        
        
<article class="max-w-5xl mx-auto">
    <header class="mb-14">
        <h1 class="text-3xl text-center font-bold leading-normal text-gray-900 mt-0 mb-3">[필기] 컴퓨터공학과 &lt;인공지능&gt; 과목 - 후반부</h1>
        <div class="text-center">Published on 23 April 2023 11:06 AM</div>
        
        <div class="mt-3 text-center">
            
            <a href="/tags/❮필기❯" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#❮필기❯</a>
            
            <a href="/tags/인공지능" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#인공지능</a>
            
            <a href="/tags/머신러닝" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#머신러닝</a>
            
            <a href="/tags/파이썬" class="inline-block bg-gray-200 rounded-full px-3 py-1 text-sm font-medium text-gray-700 m-0.5">#파이썬</a>
            
        </div>
        
        
        <div class="mt-10 -mx-7 md:mx-0">
            <img class="w-full max-w-2xl mx-auto" src="/assets/img/neural-network.jpg" width="960" height="500" alt="This post thumbnail">
        </div>
        
    </header>
    <div id="content" class="prose text-gray-800 max-w-none">
        <h1>12. Backward propagation</h1>
<h2>12-1. 역전파 오차 계산법</h2>
<pre><code> 입력층(0)                    은닉층(1)                       출력층(2)
────────────────────────────────────────────────────────────────────────────────
   x₁  ─────&gt; w₁₁⁽¹⁾  ↗  z₁⁽¹⁾ | a₁⁽¹⁾  ───↘ w₁₁⁽²⁾  ↘                  e₁⁽²⁾
       ╲ \ ↗  w₂₁⁽¹⁾  ↗                 ╲ ↗  w₁₂⁽²⁾  ↗  z₁⁽²⁾ | a₁⁽²⁾  →    
        ╲ ╱↘  w₁₂⁽¹⁾  ↘                 ╱╲ ↗ w₂₁⁽²⁾  ↗ 
         ╳               z₂⁽¹⁾ | a₂⁽¹⁾    ╳     
        ╱ ╲↗  w₂₂⁽¹⁾  ↗                 ╲╱ ↘ w₂₂⁽²⁾  ↘                  e₂⁽²⁾
       ╱ / ↘  w₁₃⁽¹⁾  ↘                 ╱ ↘  w₃₁⁽²⁾  ↘  z₂⁽²⁾ | a₂⁽²⁾  →
   x₂  ─────&gt; w₂₃⁽¹⁾  ↘      Σ | ∫      ───↗ w₃₂⁽²⁾  ↗
────────────────────────────────────────────────────────────────────────────────
   ↑            ↑          ↑       ↑    ?      ↑           ↑       ↑    ↑
   X = A⁽⁰⁾    W⁽¹⁾       Z⁽¹⁾    A⁽¹⁾ E⁽¹⁾   W⁽²⁾        Z⁽²⁾    A⁽²⁾ E⁽²⁾
</code></pre>
<ul>
<li>지금까지는 오차 E를 출력층에서만 구할 수 있었음
<ul>
<li>y와 ŷ의 차이가 바로 오차 E⁽²⁾ 가 되기 때문</li>
<li>하지만 은닉층에서의 오차인 E⁽¹⁾는 측정할 수 없었음</li>
<li>출력층의 오차를 통해 은닉층의 오차를 '역추적'하는 것이 바로 '오차 역전파'라는 개념
<ul>
<li>E⁽²⁾를 통해 E⁽¹⁾를 추정할 수 있다면 가중치를 더욱 정밀하게 조정할 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>e.g. 은닉층에서도 y와 ŷ을 계산할 수 있다고 가정해보기<pre><code> 입력층(0)                 은닉층(1)                    출력층(2)
───────────────────────────────────────────────────────────────────
  1.0  ─────&gt;  0.1  ↗  z₁⁽¹⁾ | a₁⁽¹⁾  ───↘  0.3  ↘
       ╲ \ ↗   0.4  ↗                 ╲ ↗   0.2  ↗  z₁⁽²⁾ | a₁⁽²⁾
        ╲ ╱↘   0.2  ↘                 ╱╲ ↗  0.1  ↗ 
         ╳             z₂⁽¹⁾ | a₂⁽¹⁾    ╳     
        ╱ ╲↗   0.2  ↗                 ╲╱ ↘  0.2  ↘   
       ╱ / ↘   0.3  ↘                 ╱ ↘   0.1  ↘  z₂⁽²⁾ | a₂⁽²⁾
  0.0  ─────&gt;  0.0  ↘      Σ | ∫      ───↗  0.1  ↗
───────────────────────────────────────────────────────────────────
</code></pre>
<ul>
<li>y₁ = 1 이고 ŷ₁ = 0.58 이라고 가정을 해보면,
<ul>
<li>e₁⁽²⁾ = 1 - 0.58 = 0.42</li>
<li>이 0.42라는 오차를 앞 가중치 w₁₁⁽²⁾, w₁₂⁽²⁾, w₂₁⁽²⁾에 적절히 분배해야 함
<ul>
<li>3분의 1로 균등하게 배분하는 대신, 가중치가 오차에 영향을 끼친 정도를 고려해서, 즉 가중치에 비례하게(weighted) 배분을 해야 함</li>
</ul>
</li>
</ul>
</li>
<li>y₂ = 1 이고 ŷ₂ = 0.55 이라고 가정을 해보면,
<ul>
<li>e₂⁽²⁾ = 1 - 0.55 = 0.45</li>
</ul>
</li>
<li>오차 역전파 계산<pre><code>e₁⁽²⁾  = 0.42 = e₁₁⁽²⁾ + e₁₂⁽²⁾ + e₂₁⁽²⁾

e₁⁽²⁾을 가중치에 비례하게 분배하면,        

e₁₁⁽²⁾ = 0.42 × { 0.3 / ( 0.3 + 0.2 + 0.1) } = 0.21
e₁₂⁽²⁾ = 0.42 × { 0.2 / ( 0.3 + 0.2 + 0.1) } = 0.14
e₂₁⁽²⁾ = 0.42 × { 0.1 / ( 0.3 + 0.2 + 0.1) } = 0.07

마찬가지로

e₂⁽²⁾  = 0.45 = e₂₂⁽²⁾ + e₃₁⁽²⁾ + e₃₂⁽²⁾

e₂⁽²⁾을 가중치에 비례하게 분배하면,

e₂₂⁽²⁾ = 0.45 × { 0.2 / ( 0.2 + 0.1 + 0.1) } = 0.23
e₃₁⁽²⁾ = 0.45 × { 0.1 / ( 0.2 + 0.1 + 0.1) } = 0.11
e₃₂⁽²⁾ = 0.45 × { 0.1 / ( 0.2 + 0.1 + 0.1) } = 0.11

        ┌ e₁⁽²⁾ ┐     ┌ 0.42 ┐
E⁽²⁾  = │       │  =  │      │
        └ e₂⁽²⁾ ┘     └ 0.45 ┘

이렇게 구한 E⁽²⁾로 E⁽¹⁾를 구하려면

        ┌ e₁⁽¹⁾ ┐     ┌ { e₁⁽²⁾ × ( w₁₁ / ³Σᵢ₌₁wᵢ₁ ) } + { e₂⁽²⁾ × ( w₁₂ / ³Σᵢ₌₁wᵢ₂ ) } ┐
E⁽¹⁾  = │ e₂⁽¹⁾ │  =  │ { e₁⁽²⁾ × ( w₂₁ / ³Σᵢ₌₁wᵢ₁ ) } + { e₂⁽²⁾ × ( w₂₂ / ³Σᵢ₌₁wᵢ₂ ) } │
        └ e₃⁽¹⁾ ┘     └ { e₁⁽²⁾ × ( w₃₁ / ³Σᵢ₌₁wᵢ₁ ) } + { e₂⁽²⁾ × ( w₃₂ / ³Σᵢ₌₁wᵢ₂ ) } ┘

        ┌ 0.44 ┐  
      = │ 0.25 │  
        └ 0.18 ┘

하지만 이를 단순화 할 수도 있는데, e₁⁽²⁾과 e₂⁽²⁾를 곱셈으로 따로 분리하면, 분모인 가중치의 합 Σ 부분을 상수 취급할 수 있게 됨

        ┌ e₁⁽¹⁾ ┐     ┌ { e₁⁽²⁾ × ( w₁₁ / ³Σᵢ₌₁wᵢ₁ ) } + { e₂⁽²⁾ × ( w₁₂ / ³Σᵢ₌₁wᵢ₂ ) } ┐
E⁽¹⁾  = │ e₂⁽¹⁾ │  =  │ { e₁⁽²⁾ × ( w₂₁ / ³Σᵢ₌₁wᵢ₁ ) } + { e₂⁽²⁾ × ( w₂₂ / ³Σᵢ₌₁wᵢ₂ ) } │
        └ e₃⁽¹⁾ ┘     └ { e₁⁽²⁾ × ( w₃₁ / ³Σᵢ₌₁wᵢ₁ ) } + { e₂⁽²⁾ × ( w₃₂ / ³Σᵢ₌₁wᵢ₂ ) } ┘

        ┌ w₁₁ / ³Σᵢ₌₁wᵢ₁ + w₁₂ / ³Σᵢ₌₁wᵢ₂ ┐   ┌ e₁⁽²⁾ ┐
      = │ w₂₁ / ³Σᵢ₌₁wᵢ₁ + w₂₂ / ³Σᵢ₌₁wᵢ₂ │ × │       │
        └ w₃₁ / ³Σᵢ₌₁wᵢ₁ + w₃₂ / ³Σᵢ₌₁wᵢ₂ ┘   └ e₂⁽²⁾ ┘

        ┌ w₁₁   w₁₂ ┐   ┌ e₁⁽²⁾ ┐
      = │ w₂₁   w₂₂ │ × │       │
        └ w₃₁   w₃₂ ┘   └ e₂⁽²⁾ ┘
</code></pre>
</li>
<li>ΣE⁽²⁾ = e₁⁽²⁾ + e₂⁽²⁾ = 0.87 = 0.44 + 0.25 + 0.18 = ΣE⁽¹⁾</li>
<li>따라서 오차 역전파를 통해 각 층의 오차를 구하는 일반 식은 다음과 같음<pre><code>       ┌ w₁₁  ...  w₁ₘ ┐   ┌ e₁⁽ˡ⁺¹⁾ ┐
E⁽ˡ⁾ = │ ...  ...  ... │ × │   ...   │ = W⁽ˡ⁺¹⁾ᵀ × E⁽ˡ⁺¹⁾
       └ wₙ₁  ...  wₙₘ ┘   └ eₙ⁽ˡ⁺¹⁾ ┘

</code></pre>
</li>
</ul>
</li>
</ul>
<h3>역전파의 오차함수 미분 - wⱼₖ를 먼저 미분하기</h3>
<ul>
<li>역전파에서의 가중치 조정값: W<sub>new</sub>⁽²⁾ = W<sub>old</sub>⁽²⁾ - αΔW⁽²⁾ = W<sub>old</sub>⁽²⁾ - α(∂E/∂W⁽²⁾)
<ul>
<li>역전파에서는 오차함수를 J가 아니라 E로 표기함</li>
<li>역전파에서의 α는 학습률, 즉 η를 의미함</li>
<li>경사하강법과 동일하게, 가중치 W를 조정해서 오차인 E를 최소화하는것이 목적</li>
</ul>
</li>
<li>가중치 W는 행렬인데, 행렬을 어떻게 미분해야 할까?
<ul>
<li>행렬의 미분은 까다로우므로, 오차함수를 wⱼₖ⁽²⁾에 대해 미분해서 그 결과로 W의 미분을 유추해보기
<ul>
<li>wⱼₖ⁽²⁾ = 은닉층 노드 j와 출력층 노드 k 사이의 가중치
<ul>
<li>wⱼₖ⁽²⁾ = wⱼₖ⁽²⁾ - αΔwⱼₖ⁽²⁾ = wⱼₖ⁽²⁾ - α(∂E/∂wⱼₖ⁽²⁾)</li>
<li>여기서 오차함수 E는 아달라인 비용함수와 유사함
<ul>
<li>∂E/∂wⱼₖ⁽²⁾ = ∂/∂wⱼₖ × 1/2 × ⁿΣₘ₌₁(yₘ - ŷₘ)²</li>
</ul>
</li>
</ul>
</li>
<li>wⱼₖ⁽²⁾와 연결된 노드와 가중치에 대해서만 편미분을 진행할 예정
<ul>
<li>모든 노드에 대해서 진행할 필요가 없으므로, ⁿΣₘ₌₁는 제거해버려도 상관없음</li>
<li>즉, ∂E/∂wⱼₖ⁽²⁾ = ∂/∂wⱼₖ × 1/2 × (yₖ - ŷₖ)²</li>
<li>여기에 합성함수의 미분법 f(g(x))' = f'(g(x))g'(x)를 적용하게 되면,
<ul>
<li>∂E/∂wⱼₖ⁽²⁾ = 1/2 × 2(yₖ - ŷₖ) × ∂/∂wⱼₖ × (yₖ - ŷₖ)</li>
<li>yₖ는 출력층 k의 클래스 레이블이며, wⱼₖ가 변하더라도 함께 따라서 변하지 않는 고정된 상수이므로 소거시켜버림
<ul>
<li>∂E/∂wⱼₖ⁽²⁾ = (yₖ - ŷₖ) × ∂/∂wⱼₖ × -ŷₖ = -(yₖ - ŷₖ) × ∂ŷₖ/∂wⱼₖ
<ul>
<li>∂ŷₖ/∂wⱼₖ에서의 ŷₖ는 출력층의 출력인 aₖ⁽²⁾와 동일하며, aₖ⁽²⁾는 zₖ⁽²⁾에 활성화 함수를 적용한 값이므로,</li>
<li>∂ŷₖ/∂wⱼₖ = ∂aₖ⁽²⁾/∂wⱼₖ = ∂/∂wⱼₖ × g(zₖ⁽²⁾)</li>
</ul>
</li>
<li>따라서 ∂E/∂wⱼₖ⁽²⁾ = -(yₖ - ŷₖ) × ∂/∂wⱼₖ × g(zₖ⁽²⁾)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>오차 E를 wⱼₖ⁽²⁾로 편미분하는 본론으로 되돌아가면,
<ul>
<li>∂E/∂wⱼₖ⁽²⁾ = -(yₖ - ŷₖ) × ∂/∂wⱼₖ × g(zₖ⁽²⁾)</li>
<li>여기에 합성함수 미분법인 f(g(x))' = f'(g(x))g'(x)를 적용하게 되면,
<ul>
<li>∂E/∂wⱼₖ⁽²⁾ = -(yₖ - ŷₖ) × g'(zₖ⁽²⁾) × ∂zₖ⁽²⁾/∂wⱼₖ
<ul>
<li>여기서 zₖ⁽²⁾는 출력층 k 노드의 순입력이고, 이를 계산하면 Σⱼ(wⱼₖ × aⱼ)가 됨
<ul>
<li>여기서도 wⱼₖ와 상관없는 것들은 고정된 상수 취급해서 없애버리면,
<ul>
<li>∂E/∂wⱼₖ⁽²⁾ = -(yₖ - ŷₖ) × g'(zₖ⁽²⁾) × aⱼ</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Δwⱼₖ⁽²⁾ = ∂E/∂wⱼₖ⁽²⁾ = -(yₖ - ŷₖ) × g'(zₖ⁽²⁾) × aⱼ
<ul>
<li>-(yₖ - ŷₖ) = 오차 = 출력층 k 노드에서의 레이블과 예측값과의 차이</li>
<li>g'(zₖ⁽²⁾) = 활성화 함수를 미분해서 zₖ⁽²⁾를 적용시킨 값
<ul>
<li>g'(zₖ⁽²⁾) 자리에다가 활성화 함수를 대입하면 됨</li>
</ul>
</li>
<li>aⱼ = 은닉층 노드 j의 출력</li>
</ul>
</li>
<li>하지만 위의 식은 Δwⱼₖ⁽²⁾에 대한 미분값이지 실제 W⁽²⁾에 대한 미분값은 아직 아님
<ul>
<li>이 방식을 wⱼₖ과 연결된 노드 외에도 전부 적용해주면 됨
<ul>
<li>(yₖ - ŷₖ) = 출력층의 오차 = E⁽²⁾</li>
<li>zₖ⁽²⁾ = 출력층의 순입력 = Z⁽²⁾</li>
<li>aⱼ = 은닉층의 출력 = A⁽¹⁾에다가 전치를 적용해서 형상을 맞춰줘야 함 = A⁽¹⁾ᵀ</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>ΔW⁽²⁾ = ∂E/∂W⁽²⁾ = -E⁽²⁾ × g'(Z⁽²⁾) × A⁽¹⁾ᵀ
<ul>
<li>이 식으로 출력층의 가중치를 조정할 수 있게 됨</li>
<li>동일한 방식으로 W⁽¹⁾도 계산할 수 있음
<ul>
<li>ΔW⁽¹⁾ = ∂E/∂W⁽¹⁾ = -E⁽¹⁾ × g'(Z⁽¹⁾) × A⁽⁰⁾ᵀ</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>W⁽²⁾ = W⁽²⁾ - αΔW⁽²⁾ = W⁽²⁾ - α(∂E/∂W⁽²⁾) = W⁽²⁾ + {αE⁽²⁾ × g'(Z⁽²⁾) × A⁽¹⁾ᵀ}</li>
<li>W⁽¹⁾ = W⁽¹⁾ - αΔW⁽¹⁾ = W⁽¹⁾ - α(∂E/∂W⁽¹⁾) = W⁽¹⁾ + {αE⁽¹⁾ × g'(Z⁽¹⁾) × A⁽⁰⁾ᵀ}</li>
<li>이것이 바로 신경망에서의 오차의 역전파 및 가중치 조정</li>
</ul>
<h3>역전파의 오차함수 미분 - 행렬을 직접 미분</h3>
<ul>
<li>F(x) = f(g(x))일 때, x가 변하면 g(x)도 변하고 f(g(x))도 변하고 F(x)도 변함
<ul>
<li>이를 '연쇄법칙'이라고 함</li>
</ul>
</li>
<li>y = f(u), u = g(x)일 때,
<ul>
<li>F'(x) = f'(g(x))g'(x) 라고 표기하는 것은 '라그랑주 표기법',</li>
<li>dy/dx = dy/du × du/dx 라고 표기하는 것은 '라이프니츠 표기법'이라고 함</li>
</ul>
</li>
<li>위에서 언급된 역전파의 오차함수 미분을 행렬의 미분을 통해 수행해보자면,
<ul>
<li>ΔW⁽²⁾ = ∂E/∂W⁽²⁾
<ul>
<li>출력층의 오차값을 나타내는 행렬 E를 W⁽²⁾에 대해 편미분하기</li>
<li>행렬 E는 출력층의 출력 A⁽²⁾로부터 나옴</li>
<li>따라서 연쇄법칙에 의해 ΔW⁽²⁾ = ∂E/∂W⁽²⁾ = ∂E/∂A⁽²⁾ × ∂A⁽²⁾/∂W⁽²⁾
<ul>
<li>A⁽²⁾는 순입력 Z⁽²⁾에 활성화 함수를 적용해서 나옴</li>
<li>따라서 연쇄법칙에 의해 ΔW⁽²⁾ = ∂E/∂W⁽²⁾ = ∂E/∂A⁽²⁾ × ∂A⁽²⁾/∂W⁽²⁾ = ∂E/∂A⁽²⁾ × ∂A⁽²⁾/∂Z⁽²⁾ × ∂Z⁽²⁾/∂W⁽²⁾
<ul>
<li>W⁽²⁾로 미분할 식은 이미 전부 나왔기 때문에 여기에서 연쇄법칙은 멈춤</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>ΔW⁽²⁾ = ∂E/∂W⁽²⁾ = ∂E/∂A⁽²⁾ × ∂A⁽²⁾/∂W⁽²⁾ = ∂E/∂A⁽²⁾ × ∂A⁽²⁾/∂Z⁽²⁾ × ∂Z⁽²⁾/∂W⁽²⁾
<ul>
<li>∂E/∂A⁽²⁾ 부분을 먼저 미분해보기
<ul>
<li>E = 1/2(A⁽²⁾-Y)² 이고, 이를 A⁽²⁾로 단순 미분하게 되면 A⁽²⁾-Y 가 됨</li>
<li>즉, ∂E/∂A⁽²⁾ = E⁽²⁾</li>
</ul>
</li>
<li>∂A⁽²⁾/∂Z⁽²⁾ 부분을 미분해보기
<ul>
<li>A⁽²⁾ = g(Z⁽²⁾) 이므로, ∂A⁽²⁾/∂Z⁽²⁾ = g'(Z⁽²⁾)</li>
<li>활성화 함수 g(x)는 다양하게 존재하므로, 현재 상황에서는 미분을 실제로 진행할수는 없고, 미분이 필요하다는 라그랑주 표기만 해둠</li>
</ul>
</li>
<li>∂Z⁽²⁾/∂W⁽²⁾ 부분을 미분해보기
<ul>
<li>Z⁽²⁾ = W⁽²⁾ × A⁽¹⁾ 이므로, 순입력 Z⁽²⁾를 가중치 W⁽²⁾에 대해 미분하면 A⁽¹⁾만 남게 됨</li>
<li>다만 앞의 행렬과 연산을 수행하려면 형상을 맞춰줘야 하기 때문에 전치가 필요함</li>
<li>따라서 ∂Z⁽²⁾/∂W⁽²⁾ = A⁽¹⁾ᵀ</li>
</ul>
</li>
<li>이제 미분을 수행한 부분들을 원래의 식에 대입하면 다음과 같은 최종 식이 산출됨</li>
</ul>
</li>
<li>ΔW⁽²⁾ = ∂E/∂W⁽²⁾ = E⁽²⁾ × g'(Z⁽²⁾) × A⁽¹⁾ᵀ
<ul>
<li>이 식은 위에서 wⱼₖ를 먼저 미분하고 이를 통해 W를 미분한 식과 동일함</li>
<li>행렬을 직접 미분하면 역전파 오차함수의 미분을 훨씬 간단하게 산출할 수 있음</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>12-2. 객체지향으로 구현한 XOR 게이트 신경망</h2>
<ul>
<li>XOR 게이트<pre><code>x₁ ───┬──&gt;┌      ┐ 
     ┌┼──&gt;│ NAND │ h₁ 
     ││   └      ┘ ↘
     ││               [ AND ] → y
     ││   ┌      ┐ ↗
     │└──&gt;│  OR  │ h₂
x₂ ──┴───&gt;└      ┘ 
</code></pre>
<ul>
<li>입력 자료 X
<ul>
<li>특성의 개수 = 2
<ul>
<li>TRUE or FALSE (1 or 0)</li>
</ul>
</li>
<li>샘플의 개수 = 4
<ul>
<li>x₁와 x₂가 각각 0과 1의 값을 가짐</li>
</ul>
</li>
<li>XOR 게이트의 입력값 X = (2 × 4)의 형상을 가지는 행렬
<ul>
<li>truth table을 생각해보면 됨<pre><code>X = ┌ 0  0  1  1 ┐  
    └ 0  1  0  1 ┘ 
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
<li>클래스 레이블 Y
<ul>
<li>입력에 대해 TRUE 또는 FALSE의 값을 출력</li>
<li>XOR 게이트의 출력값 Y = (1 × 4)의 형상을 가지는 행렬
<ul>
<li>truth table을 생각해보면 됨<pre><code>Y = ( 0  1  1  0 )
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>XOR 게이트를 신경망으로<pre><code>    입력층(0)                   은닉층(1)                      출력층(2)
─────────────────────────────────────────────────────────────────────────────
      x₁  ─────&gt; w₁₁⁽¹⁾  ↗  z₁⁽¹⁾ | a₁⁽¹⁾  ╲     w₁₁⁽²⁾  ↘
          ╲ \ ↗  w₂₁⁽¹⁾  ↗                  ╲      
           ╲ ╱↘  w₁₂⁽¹⁾  ↘                   ↘   
            ╳               z₂⁽¹⁾ | a₂⁽¹⁾  ────&gt; w₂₁⁽²⁾  →   z₁⁽²⁾ | a₁⁽²⁾
           ╱ ╲↗  w₂₂⁽¹⁾  ↗                   ↗          
          ╱ / ↘  w₁₃⁽¹⁾  ↘                  ╱      
      x₂  ─────&gt; w₂₃⁽¹⁾  ↘      Σ | ∫      ╱     w₃₁⁽²⁾  ↗
─────────────────────────────────────────────────────────────────────────────
      ↑            ↑          ↑       ↑           ↑           ↑       ↑   
      X = A⁽⁰⁾    W⁽¹⁾       Z⁽¹⁾    A⁽¹⁾        W⁽²⁾        Z⁽²⁾    A⁽²⁾
      ↑                           ↑                                ↑
 입력 노드 수는               은닉층 노드 수는                        출력값은 0
 특성만큼 존재함             임의로 결정해야 함                      아니면 1이므로
 노드 개수는 2개          여기서는 최소 3개로 지정                   노드 개수는 1개
</code></pre>
<ul>
<li>입력층의 형상 파악하기
<ul>
<li>A⁽⁰⁾의 형상은 (2, 4)
<ul>
<li>위에서 언급한대로 (특성의 개수 × 샘플의 개수)</li>
</ul>
</li>
<li>W⁽¹⁾의 형상은 (3, 2)
<ul>
<li>(임의로 지정한 은닉층 노드 수 × 입력층 노드 수)</li>
</ul>
</li>
</ul>
</li>
<li>은닉층의 형상 파악하기
<ul>
<li>Z⁽¹⁾의 형상은 (3, 4)
<ul>
<li>W⁽¹⁾ × A⁽⁰⁾ = Z⁽¹⁾이므로</li>
</ul>
</li>
<li>A⁽¹⁾의 형상은 (3, 4)
<ul>
<li>A⁽¹⁾는 Z⁽¹⁾의 원소에 각각 활성화 함수를 적용한 것이므로 Z⁽¹⁾의 형상과 동일함</li>
</ul>
</li>
<li>W⁽²⁾의 형상은 (1, 3)
<ul>
<li>W⁽²⁾ × A⁽¹⁾ = Z⁽²⁾이고, A⁽¹⁾의 형상이 (3 × 4)인 것을 알고, Z⁽²⁾의 형상이 (1 × 4)인 것을 알고 있으므로, 이를 기반으로 W⁽²⁾의 형상을 추론하면 됨</li>
<li>(출력층 노드 수 × 은닉층 노드 수)</li>
</ul>
</li>
</ul>
</li>
<li>출력층의 형상 파악하기
<ul>
<li>Z⁽²⁾의 형상은 (1, 4)
<ul>
<li>W⁽²⁾ × A⁽¹⁾ = Z⁽²⁾이므로</li>
<li>클래스 레이블의 형상과 동일함</li>
</ul>
</li>
</ul>
</li>
<li>이렇게 형상을 먼저 파악하고 나서 코드를 작성하면 됨</li>
</ul>
</li>
</ul>
<h3>XOR 신경망 Code Example</h3>
<pre><code>class NeuralNetwork(): 

    def __init__(self, net_arch, eta=0.1, epochs=10000, random_seed=1):
        self.layers = len(net_arch)                             # 신경망 구조를 나타내는 network architecture 변수
        self.net_arch = net_arch                                # 각 층의 노드 개수를 여기서 정해줌 ( e.g. net_arch = [2, 3, 1] )
        self.eta = eta
        self.epochs = epochs
        self.random_seed = random_seed
    
    def g(self, x):                                             # 활성화 함수 메소드
        return 1/(1+np.exp((-x)))                               # 여기서는 sigmoid 함수를 사용함

    def g_prime(self, x):                                       # 활성화 함수를 미분하는 메소드
        return self.g(x) * (1-self.g(x))

    def fit(self, X, Y):
        np.random.seed(self.random_seed)
        W1_shape = (self.net_arch[1], self.net_arch[0])         # 은닉층이 하나만 있다고 가정
        W2_shape = (self.net_arch[2], self.net_arch[1])
        self.W1 = 2*np.random.random(W1_shape) - 1              # 0 ~ 1 대신 -1 ~ 1 범위를 갖는 임의의 숫자를 가중치로 부여함
        self.W2 = 2*np.random.random(W2_shape) - 1
    
        self.cost_ = []                                         # 출력층 에러 E⁽²⁾를 여기에 저장하게 됨

        for _ in range(self.epochs):                            

            # 순전파를 시작함
            A0 = X                                              # A⁽⁰⁾ = 입력 X
            Z1 = np.dot(self.W1, A0)                            # 은닉층 순입력 Z⁽¹⁾ = W⁽¹⁾ × A⁽⁰⁾
            A1 = self.g(Z1)                                     # Z⁽¹⁾에다가 활성화 함수 g(x)를 적용한 것이 은닉층 출력값
            Z2 = no.dot(self.W2, A1)                            # 출력층 순입력 Z⁽²⁾ = W⁽²⁾ × A⁽¹⁾
            A2 = self.g(Z2)                                     # Z⁽²⁾에다가 활성화 함수 g(x)를 적용한 것이 최종 출력값

            # 역전파를 사용해서 신경망의 가중치를 조정함
            E2 = Y - A2                                         # E⁽²⁾ = Y - A⁽²⁾
            E1 = np.dot(self.W2.T, E2)                          # E⁽¹⁾ = W⁽²⁾ᵀ × E⁽²⁾

            dZ2 = E2 * self.g_prime(Z2)                         # W⁽²⁾ = W⁽²⁾ + {αE⁽²⁾ × g'(Z⁽²⁾) × A⁽¹⁾ᵀ} 에서 E⁽²⁾ × g'(Z⁽²⁾) 부분을 별개로 dZ2로 정의함
            dZ1 = E1 * self.g_prime(Z1)                         # W⁽¹⁾ = W⁽¹⁾ + {αE⁽¹⁾ × g'(Z⁽¹⁾) × A⁽⁰⁾ᵀ} 에서 E⁽¹⁾ × g'(Z⁽¹⁾) 부분을 별개로 dZ1로 정의함

            self.W2 += np.dot(dZ2, A1.T)                        # 새로운 가중치 W⁽²⁾ = 기존 가중치 W⁽²⁾ + {αE⁽²⁾ × g'(Z⁽²⁾) × A⁽¹⁾ᵀ}
            self.W1 += np.dot(dZ1, A0.T)                        # 새로운 가중치 W⁽¹⁾ = 기존 가중치 W⁽¹⁾ + {αE⁽¹⁾ × g'(Z⁽¹⁾) × A⁽⁰⁾ᵀ}

            self.cost_.append(np.sqrt(np.sum(E2*E2)))           # 오차가 음수로 나오는 것을 방지하기 위해서 E⁽²⁾를 제곱하여 루트를 씌움
            
        return self
    
    def net_input(self, X):
        if X.shape[0] == self.w.shape[0]: 
            return np.dot(X, self.w)
        else:
            return np.dot(X, self.w[1:]) + self.w[0]
        
    def predict(self, X):                                       # 반복 학습은 필요하지 않고, 순전파 결과를 바탕으로 분류를 파악하는 용도로만 사용됨
        Z1 = np.dot(self.W1, X)
        A1 = self.g(Z1)
        Z2 = np.dot(self.W2, A1)
        A2 = self.g(Z2)
        return A2
</code></pre>
<ul>
<li>만약 출력 결과가 정확하지 않다면 학습률을 조정하거나, 에폭수를 늘리거나, 은닉층 개수를 조정해볼 수 있음</li>
</ul>
<h2>12-3. 행렬을 통해 바라본 순전파와 역전파</h2>
<ul>
<li>[2, 3, 1] 구조의 XOR 게이트를 예로 들자면,<pre><code>    입력층(0)                   은닉층(1)                       출력층(2)
────────────────────────────────────────────────────────────────────────────────
      x₁  ─────&gt; w₁₁⁽¹⁾  ↗  z₁⁽¹⁾ | a₁⁽¹⁾  ╲     w₁₁⁽²⁾  ↘
          ╲ \ ↗  w₂₁⁽¹⁾  ↗                  ╲      
           ╲ ╱↘  w₁₂⁽¹⁾  ↘                   ↘   
            ╳               z₂⁽¹⁾ | a₂⁽¹⁾  ────&gt; w₂₁⁽²⁾  →   z₁⁽²⁾ | a₁⁽²⁾
           ╱ ╲↗  w₂₂⁽¹⁾  ↗                   ↗          
          ╱ / ↘  w₁₃⁽¹⁾  ↘                  ╱      
      x₂  ─────&gt; w₂₃⁽¹⁾  ↘      Σ | ∫      ╱     w₃₁⁽²⁾  ↗
────────────────────────────────────────────────────────────────────────────────
      ↑            ↑         ↑         ↑           ↑          ↑         ↑   
      X           W⁽¹⁾      Z⁽¹⁾      A⁽¹⁾        W⁽²⁾       Z⁽²⁾      A⁽²⁾
      =                      =         =                      =         =
     A⁽⁰⁾                W⁽¹⁾×A⁽⁰⁾   g(Z⁽¹⁾)              W⁽²⁾×A⁽¹⁾   g(Z⁽²⁾)

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; 순전파 &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;

&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; 역전파 &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;
                   ↑                               ↑
                  W⁽¹⁾                            W⁽²⁾
                   =                               =
  W⁽¹⁾ + {αE⁽¹⁾ × g'(Z⁽¹⁾) × A⁽⁰⁾ᵀ}     W⁽²⁾ + {αE⁽²⁾ × g'(Z⁽²⁾) × A⁽¹⁾ᵀ}
</code></pre>
</li>
</ul>
<hr>
<h1>13. Logistic Regression Analysis</h1>
<h2>13-1. 회귀분석이란?</h2>
<ul>
<li>회귀란 변수들 사이의 관계를 분석하는 것</li>
<li>회귀분석이란 회귀를 통한 분석의 결과를 바탕으로 함수를 정의해서 값을 예측하는 기법
<ul>
<li>단순 회귀분석: x와 y 두개의 변수를 분석하는 것</li>
<li>다중 회귀분석: 두개 이상의 변수들을 분석하는</li>
<li>선형 회귀분석: 입력값 x에 대한 연속적인 y값 중에서 하나의 값을 예측하는 것
<ul>
<li>Z = W × X 형태의 함수(직선 그래프)를 사용함</li>
<li>e.g. 지난 20년간 서울시 마포구의 아파트 시세 데이터를 바탕으로, 3년 뒤의 아파트 시세를 예측하는 것</li>
</ul>
</li>
<li>로지스틱 회귀분석: 입력값 x에 대해서 y의 값이 특성과 일치하는지(1) 아닌지(0) 그 확률을 예측하는 것
<ul>
<li>true/false, 0/1, pass/fail과 같이 두 종류의 결과값에 입력값이 얼마나 근접한지를 판단</li>
<li>e.g. 수신되는 메일이 스팸 메일에 가까운지 정상적인 메일에 가까운지를 판단해서 자동으로 메일함에 뷴류하는 시스템</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Logistic function</h3>
<ul>
<li>로지스틱 함수란 입력값을 0~1 범위의 출력값(확률)으로 변환해주는 함수
<ul>
<li>로지스틱 회귀분석의 특징을 보면 알겠지만, 주로 sigmoid 함수가 사용됨
<ul>
<li>'시그모이드'라는 명칭은 입력값을 0~1 범위로 출력해주는 다양한 함수들을 통칭하는데 사용하기도 함</li>
</ul>
</li>
<li>로지스틱 회귀분석에서 사용하는 로지스틱 함수는 우리가 알고 있는 시그모이드 함수와 유사한 다음과 같은 '로지스틱 시그모이드' 함수를 사용함
<ul>
<li>h(z) = 1 / (1 + e⁻ʷ˙ˣ) = 1 / (1 + e⁻ᶻ)</li>
<li>시그모이드 함수에서 입력값 x를 신경망 순입력 z로 대체한 형태</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>회귀 모델</h3>
<ul>
<li>y = h(z) = 1 / (1 + e⁻ʷ˙ˣ) 에서,
<ul>
<li>if (W × X → ∞), y → 1</li>
<li>if (W × X → 0), y → 1/2</li>
<li>if (W × X → -∞), y → 0</li>
</ul>
</li>
<li>로지스틱 함수의 입력 z는 반드시 선형일 필요는 없음
<ul>
<li>고차원 다항식도 가능함
<ul>
<li>e.g. z = w₀ + w₁x₁²+ w₂x₂²
<ul>
<li>이 경우에는 결정경계선이 직선 그래프가 아닌 원형으로 나오게 됨</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>13-2. 로지스틱 회귀분석에서의 비용함수와 그 미분</h2>
<ul>
<li>선형 회귀분석에서의 오차함수 E
<ul>
<li>오차제곱합 Sum of Squared Error(SSE) = ᵐΣᵢ₌₁ 1/2(y⁽ⁱ⁾ - ŷ⁽ⁱ⁾)²
<ul>
<li>평균제곱오차 Mean of Squared Error(MSE) = 1/m × ᵐΣᵢ₌₁ 1/2(y⁽ⁱ⁾ - ŷ⁽ⁱ⁾)² = 1/2m × ᵐΣᵢ₌₁ (y⁽ⁱ⁾ - ŷ⁽ⁱ⁾)²
<ul>
<li>자료가 많을 경우 overflow가 발생할 수 있으므로 샘플 개수 m만큼 나누어야 함</li>
</ul>
</li>
</ul>
</li>
<li>선형 회귀의 출력값 y는 WX지만, 로지스틱 회귀의 출력은 로지스틱 함수이므로 이 오차함수를 그대로 적용할 수 없음</li>
</ul>
</li>
<li>로지스틱 회귀분석에서의 오차함수, 즉 비용함수 J
<ul>
<li>if (y = 1), J = -log(h(z))
<ul>
<li>if (y = 1), e⁽ⁱ⁾ = -log(a⁽²⁾⁽ⁱ⁾)</li>
</ul>
</li>
<li>if (y = 0), J = -log(1-h(z))
<ul>
<li>if (y = 0), e⁽ⁱ⁾ = -log(1-a⁽²⁾⁽ⁱ⁾)</li>
</ul>
</li>
<li>여기서 a⁽²⁾⁽ⁱ⁾는 출력층[2]의 i번째 노드의 출력을 의미함</li>
</ul>
</li>
<li>y가 1인 경우와 0인 경우를 따로 구분하지 않고 사용할 수 있는 단일 식은 다음과 같이 표현됨
<ul>
<li>e⁽ⁱ⁾ = {-y × log(a⁽²⁾⁽ⁱ⁾)} - {(1-y) × log(1-a⁽²⁾⁽ⁱ⁾)}
<ul>
<li>y가 1인 경우 두번째 항이 0이 되어버리고</li>
<li>y가 0인 경우 첫번째 항이 0이 되어버리므로</li>
</ul>
</li>
</ul>
</li>
<li>하나의 샘플이 아닌 모든 샘플 m에 대한 비용함수 J(w)
<ul>
<li>J(w) = -1/m × ᵐΣᵢ₌₁ [(y⁽ⁱ⁾ × log(a⁽²⁾⁽ⁱ⁾)) + {(1-y⁽ⁱ⁾) × log(1-a⁽²⁾⁽ⁱ⁾)}]
<ul>
<li>정보이론에서의 cross entropy와 유사한 형태로 나옴
<ul>
<li>교차 엔트로피 J(h(z), y) = -Σᵢ(y⁽ⁱ⁾log(ŷ⁽ⁱ⁾))</li>
<li>이를 로지스틱 비용함수 J(w)에 적용을 하게 되면 다음과 같은 공식이 나옴
<ul>
<li>J(w) = -1/m × ᵐΣᵢ₌₁ {(y⁽¹⁾ × log(ŷ⁽¹⁾)) + (y⁽²⁾ × log(ŷ⁽²⁾))}</li>
</ul>
</li>
<li>즉, 로지스틱 회귀 비용함수는 '교차 엔트로피 비용함수'에 속한다고 할 수 있음</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>비용함수의 최소값을 찾기 위한 미분 수행
<ul>
<li>∂J(w)/∂wⱼ = ∂/∂wⱼ × -1/m × ᵐΣᵢ₌₁ [(y × log(a)) + {(1-y) × log(1-a)}]
<ul>
<li>y는 클래스 레이블, 0 또는 1의 값임</li>
<li>a는 출력층의 출력 = a⁽²⁾⁽ⁱ⁾ = 로지스틱 회귀 가설함수의 출력 = h(z) = σ(z)</li>
<li>z = w × x</li>
</ul>
</li>
<li>∂J(w)/∂wⱼ = ∂/∂wⱼ × -1/m × ᵐΣᵢ₌₁ [(y × log(σ(z))) + {(1-y) × log(1-σ(z))}]
<ul>
<li>d/dx × log(x) = 1/x 이므로,
<ul>
<li>∂J(w)/∂wⱼ = -1/m × ᵐΣᵢ₌₁ [(y/σ(z) × ∂σ(z)/∂wⱼ) + {(1-y)/(1-σ(z)) × ∂(1-σ(z))/∂wⱼ}]</li>
</ul>
</li>
<li>여기에 합성함수 미분법 F'(g(x)) = f'(g(x))g'(x) 을 적용하게 되면,
<ul>
<li>∂J(w)/∂wⱼ = -1/m × ᵐΣᵢ₌₁ {y/σ(z) - (1-y)/(1-σ(z))} ∂σ(z)/∂wⱼ</li>
</ul>
</li>
<li>여기에 시그모이드 함수의 미분을 이용해서 대부분의 항을 소거시키면,</li>
</ul>
</li>
<li>최종 식은 -1/m × ᵐΣᵢ₌₁ (y⁽ⁱ⁾-ŷ⁽ⁱ⁾) xⱼ⁽ⁱ⁾</li>
</ul>
</li>
<li>로지스틱 회귀 비용함수의 미분과 역전파를 통해서 가중치 조정값을 산출해내기
<ul>
<li>ΔW⁽²⁾ = ∂J/∂W⁽²⁾ = -1/m × E⁽²⁾ × 1 × A⁽¹⁾ᵀ
<ul>
<li>선형 회귀분석에서는 g'(Z⁽²⁾)였던 부분이 로지스틱 회귀분석에서는 1이 되어버림</li>
<li>출력층에서의 미분 없이 바로 오차를 역전파 할 수 있음</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>13-3. Softmax function</h2>
<ul>
<li>σ(z)ⱼ = (eᶻʲ / ᵏΣₖ₌₁ eᶻᵏ)
<ul>
<li>j = 1, 2, 3, ... , K
<ul>
<li>지수함수의 형태
<ul>
<li>미분이 가능하도록 하기 위함</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>시그모이드 함수와 비슷하게 0과 1 사이의 값으로 출력함
<ul>
<li>출력값의 총 합계를 1로 만드는 정규화를 수행하지만, 입력과 편향 뿐만 아니라 다른 뉴런들의 출력값과도 상대적인 비교를 해서 정규화를 수행함
<ul>
<li>큰 값은 더 크게, 작은 값은 더 작게 만들어서, 더 잘 구분되도록 해줌</li>
</ul>
</li>
</ul>
</li>
<li>클래스를 다중분류하는데 상당히 많이 쓰이는 활성화 함수임
<ul>
<li>e.g.
<ul>
<li>시그모이드 활성화 함수로 색상 분류를 수행한 결과값이 다음과 같은 경우
<ul>
<li>출력 1. 빨간색(0.9), 초록색(0.7), 파란색(0.8)</li>
<li>출력 2. 빨간색(0.1), 초록색(0.5), 파란색(0.3)
<ul>
<li>확실하게 어떤 색상인지를 판단하기 어려운 상태</li>
</ul>
</li>
</ul>
</li>
<li>시그모이드 대신 소프트맥스 활성화 함수로 색상 분류를 수행하면 다음과 같은 결과값이 나올 수 있음
<ul>
<li>출력 1. 빨간색(0.6), 초록색(0.1), 파란색(0.2)</li>
<li>출력 2. 빨간색(0.0), 초록색(0.6), 파란색(0.2)
<ul>
<li>차이가 조금 더 명확해짐</li>
</ul>
</li>
</ul>
</li>
<li>왜 그럴까?
<ul>
<li>두 함수 모두 결과값의 총 합계가 1이 되는 함수지만, 다른 뉴런들의 출력값도 함께 고려해 계산함
<ul>
<li>출력 1에서 빨간색이 0.6이라면, 초록색과 파란색의 두 합은 0.4가 되어야만 총 합이 1이 될 수 있음</li>
<li>다른 가중치들의 학습에 간접적인 방식으로 관여하여 미세한 차이를 크게 증폭시켜줌
<ul>
<li>경사하강법의 가속화 효과</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>출력층에서 소프트맥스 활성화 함수를 사용하게 되면 역전파의 오차를 증폭시켜 신속한 수렴이 가능해짐</li>
</ul>
</li>
</ul>
<h3>소프트맥스 함수 Code Example</h3>
<pre><code>def softmax(self, a):
    exp_a = np.exp(a - np.max(a))

    # 지수함수의 특성상 값이 급격하게 커져서 overflow가 발생할 수 있으므로,
    # exp_a를 그대로 출력하는 대신 분자 형태로 안전장치를 마련함
    return exp_a / np.sum(exp_a)
</code></pre>
<h2>13-4. Andrew Ng 교수의 로지스틱 회귀분석 실습용 planar data</h2>
<ul>
<li>실습을 위해 만들어진, 좌표평면상에 꽃 모양으로 나타나는 좌표 데이터들을 분류하는 실습
<ul>
<li>400개의 좌표 샘플</li>
<li>2개의 분류해야 할 특성
<ul>
<li>로지스틱 회귀분석을 적용해보자</li>
</ul>
</li>
</ul>
</li>
<li>만약 성능이 만족스럽지 않다면?
<ul>
<li>은닉층 노드 수를 늘리거나, 에폭 수를 늘리거나, 학습률을 조정해도 성능이 향상되지 않는다면, 활성화 함수를 교체해볼 수도 있음
<ul>
<li>시그모이드와 유사하지만, 시그모이드보다 빠르게(가파르게) 수렴하는 특징을 가지는 쌍곡탄젠트 함수도 사용 가능</li>
<li>은닉층에서는 쌍곡탄젠트 함수를 사용하고, 출력층에서는 시그모이드 함수를 사용해보기</li>
</ul>
</li>
<li>만약 활성화 함수를 교체해도 성능이 만족스럽지 않다면, 비용함수를 교체해볼 수 있음
<ul>
<li>오차제곱합(SSE) 방식의 비용함수 대신 cross entropy 비용함수를 사용
<ul>
<li>로지스틱 회귀분석에서처럼 0 또는 1로써 분류하는 경우, MSE를 사용하게 되면 틀린 샘플에 대해서 더 강화하는 특성을 보임</li>
<li>조정할 값이 점점 지나치게 작아지게 되는 문제가 발생함</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>13-5. Cross entropy</h2>
<ul>
<li>오차제곱합 기반의 비용함수 특징
<ul>
<li>목적: 함수를 최소로 하는 값을 찾기</li>
<li>방법: 모든 자료에 대한 오차의 합이 최소여야 함</li>
<li>평균제곱오차(MSE) 일반식 E = 1/2m × ᵐΣᵢ₌₁ (y⁽ⁱ⁾ - ŷ⁽ⁱ⁾)²</li>
<li>MSE code example<pre><code>def MSEcost(self, A2, Y):
    E2 = Y - A2
    cost = np.sqrt(np.sum(E2*E2))
    return cost
</code></pre>
</li>
</ul>
</li>
<li>교차 엔트로피 기반의 손실함수 특징
<ul>
<li>목적: 함수를 최소로 하는 값을 찾기</li>
<li>방법: 모든 자료의 정확한 분류여야 함</li>
<li>교차 엔트로피(CE) 오차 일반식 J = -Σᵢ(y⁽ⁱ⁾logₑ(ŷ⁽ⁱ⁾))
<ul>
<li>평균 크로스 엔트로피(ACE) 오차 일반식 J = -1/m × ΣₘΣᵢ(y⁽ⁱ⁾logₑ(ŷ⁽ⁱ⁾))</li>
</ul>
</li>
<li>CE code example<pre><code>def CEcost(self, A2, Y):
    m = Y.shape[1]                              # 샘플의 개수 m
    logprobs = np.multiply(Y, np.log(A2))
    cost = -np.sum(logprobs) / m
    cost = np.squeeze(cost)
    return cost
</code></pre>
</li>
</ul>
</li>
</ul>
<h3>교차 엔트로피 손실함수를 행렬 연산 기반으로 역전파에 적용하는 방법</h3>
<ul>
<li>은닉층 ← 출력층
<ul>
<li>전제 살펴보기
<ul>
<li>E⁽²⁾ = y - ŷ</li>
<li>dZ⁽²⁾ = E⁽²⁾ × g⁽²⁾'(Z⁽²⁾)</li>
<li>dW⁽²⁾ = dZ⁽²⁾ × A⁽¹⁾ᵀ</li>
<li>db⁽²⁾ = dZ⁽²⁾
<ul>
<li>편향에 대한 고려가 추가됨</li>
</ul>
</li>
<li>학습률은 생략함</li>
</ul>
</li>
<li>E⁽²⁾ = y - A⁽²⁾</li>
<li>dZ⁽²⁾ = E⁽²⁾ × g⁽²⁾'(Z⁽²⁾) = E⁽²⁾
<ul>
<li>교차 엔트로피 미분에서 g⁽²⁾'(Z⁽²⁾)는 1이 되므로 E⁽²⁾만 남게 됨</li>
</ul>
</li>
<li>dW⁽²⁾ = ∂E/∂W⁽²⁾ = 1/m × (E⁽²⁾ × A⁽¹⁾ᵀ) = 1/m × (dZ⁽²⁾ × A⁽¹⁾ᵀ)
<ul>
<li>전체 샘플의 개수만큼 나눠주는 작업이 추가됨</li>
</ul>
</li>
<li>db⁽²⁾ = 1/m × np.sum(dZ⁽²⁾, axis=1)
<ul>
<li>편향은 뉴런의 입력이나 가중치와 관련이 없음
<ul>
<li>오차만 역전파하면 됨</li>
</ul>
</li>
<li>db⁽²⁾ = dZ⁽²⁾ = E⁽²⁾ 이므로, E⁽²⁾를 샘플 수로 나눠주기만 하면 됨</li>
</ul>
</li>
</ul>
</li>
<li>입력층 ← 은닉층
<ul>
<li>전제 살펴보기
<ul>
<li>E⁽¹⁾ = W⁽²⁾ᵀ × E⁽²⁾</li>
<li>dZ⁽¹⁾ = E⁽¹⁾ × g⁽¹⁾'(Z⁽¹⁾)</li>
<li>dW⁽¹⁾ = dZ⁽¹⁾ × A⁽⁰⁾ᵀ</li>
<li>db⁽¹⁾ = dZ⁽¹⁾</li>
</ul>
</li>
<li>E⁽¹⁾ = W⁽²⁾ᵀ × E⁽²⁾</li>
<li>dZ⁽¹⁾ = E⁽¹⁾ ∗ g⁽¹⁾'(Z⁽¹⁾) = E⁽¹⁾ ∗ (1 - tanh²(Z⁽¹⁾)) = E⁽¹⁾ ∗ (1 - A⁽¹⁾²)
<ul>
<li>∗ 는 원소별로 각각 곱셈을 하는 연산자</li>
<li>쌍곡탄젠트 활성화 함수를 적용하는 경우임
<ul>
<li>g'(Z⁽¹⁾) = tanh'(Z⁽¹⁾) = 1 - tanh²(Z⁽¹⁾)</li>
</ul>
</li>
<li>tanh(Z1⁽¹⁾)은 은닉층의 출력 A⁽¹⁾과 같으므로 미분할 필요 없이 그대로 가져다가 쓰면 됨
<ul>
<li>tanh(Z⁽¹⁾) = g(Z⁽¹⁾) = A⁽¹⁾</li>
</ul>
</li>
</ul>
</li>
<li>dW⁽¹⁾ = dZ⁽¹⁾ × A⁽⁰⁾ᵀ</li>
<li>db⁽¹⁾ = np.sum(dZ⁽¹⁾, axis=1)</li>
</ul>
</li>
</ul>
<hr>
<h1>14. MNIST dataset을 사용한 로지스틱 회귀분석 실습</h1>
<h2>14-1. MNIST dataset의 전처리</h2>
<ul>
<li>이전에 순방향 신경망 학습에서 다루었던 바 있음
<ul>
<li>MNIST dataset은 기계학습계의 hello world
<ul>
<li>NIST의 오리지널 데이터셋인 'Special Database 1 &amp; 3'을 기계학습을 위해 재구성(modified)한 것</li>
<li>프랑스의 컴퓨터공학자 Yann André LeCun의 웹사이트 또는 TensorFlow 웹사이트에서 다운로드 받을 수 있음
<ul>
<li>http://yann.lecun.com/exdb/mnist</li>
<li>https://www.tensorflow.org/datasets/catalog/mnist</li>
</ul>
</li>
</ul>
</li>
<li>숫자 0부터 9까지를 손글씨로 적은, 28×28 픽셀 크기의 흑백 이미지
<ul>
<li>한 픽셀당 8비트의 용량을 차지함
<ul>
<li>한 픽셀은 명도 0 (검은색) 부터 명도 255 (흰색) 까지의 흑백 색상 정보를 담고 있음</li>
<li>2⁸ = 256</li>
</ul>
</li>
<li>훈련용 이미지 6만장</li>
<li>테스트용 이미지 1만장</li>
</ul>
</li>
<li>주어진 샘플인 X의 형상은 (60000, 28, 28)</li>
<li>분류의 결과물인 y의 형상은 (60000, )</li>
</ul>
</li>
<li>MNIST 데이터셋을 그대로 사용하지 않고 전처리를 해야 하는 이유
<ul>
<li>자료 크기에 따른 성능 문제가 생김
<ul>
<li>원본 자료를 그대로 사용하게 되면 연산 처리량이 상당함</li>
<li>그래서 pickle 모듈을 활용해서 부담을 덜어줌
<ul>
<li>파이썬의 객체나 자료 구조를 파일 형태로 저장할 수 있음</li>
</ul>
</li>
</ul>
</li>
<li>학습 성능을 높이기 위해서
<ul>
<li>0부터 255까지의 범위를 가지는 픽셀당 색상 정보를, 활성화 함수에 맞게 0.01부터 0.99까지의 범위로 미리 정규화해주면 성능이 올라감
<ul>
<li>0부터 1까지가 아닌 0.01부터 0.99까지인 이유는, 시그모이드 함수는 정수인 0과 1을 출력하지 못하고 그 사이값만 출력하기 때문</li>
</ul>
</li>
<li>code example<pre><code>x_std = X[:]
x_std = np.asfarray(X)/255.0 * 0.99 + 0.01
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>14-2. MNIST dataset 신경망 설계</h2>
<ul>
<li>이전에 순방향 신경망 학습에서 다루었던 바 있음
<ul>
<li>입력층의 크기: 28픽셀 × 28픽셀 = 784
<ul>
<li>입력층의 형상: (784, 1)</li>
</ul>
</li>
<li>은닉층의 크기: 여기서는 임의로 약 100개를 설정함
<ul>
<li>은닉층 형상: (100, 1)</li>
</ul>
</li>
<li>출력층의 크기: 숫자 0부터 9까지 10개로 분류해야 함 = 10
<ul>
<li>출력층 형상: (10, 1)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>One-hot-encoding</h3>
<ul>
<li>클래스 레이블이 0과 1만 가능한 XOR 신경망과는 달리, MNIST 데이터셋에 사용하는 로지스틱 회귀분석 신경망의 경우 클래스 레이블이 0부터 9까지 존재함
<ul>
<li>클래스 레이블을 나타낼 행렬이 따로 필요함</li>
</ul>
</li>
<li>해당되는 값이 있는 위치에 1을, 해당되지 않는 값이 있는 위치에 0을 부여해 인덱싱을 하는 방법
<ul>
<li>클래스 레이블의 값들을 행렬상에 나타낼 수 있음</li>
</ul>
</li>
<li>One-hot-encoding code example<pre><code>def one_hot_encoding(y, n_y, modified=True):
    y_hot = np.eye(n_y)[                            # np.eye 함수를 사용해 (n_y, n_y) 형상의 단위행렬을 만듦
        np.array(y, dtype='int32').flatten()]       # 단위행렬에서 해당 행을 추출함
    if modified:
        y_hot[y_hot == 0] = 0.01                    # 시그모이드 함수를 위해 0 대신 0.01을 사용함
        y_hot[y_hot == 1] = 0.99                    # 시그모이드 함수를 위해 1 대신 0.99를 사용함
    return y_hot
</code></pre>
</li>
</ul>
<h2>14-3. MNIST 데이터셋을 사용한 로지스틱 회귀분석 code example</h2>
<pre><code>import mlmodule                                     # 이번 실습을 위해 미리 만들어놓은 전처리용 모듈이 있다고 가정함
import numpy as np

# fit 메소드 정의
def fit(self, X, y):
    self.cost_ = []
    self.m_samples = len(y)
    Y = mlmodule.one_hot_encoding(y, self.n_y)
    for epoch in range(self.epochs):
        for sample in range(self.m_samples):
            A0 = np.array(X[sample], ndmin=2).T     # one-hot-encoding과 같은 2차원 배열이 아닌, 튜플이라던가 다른 자료형이 입력될 경우에 대비해서 전처리 해주기
            Y0 = np.array(Y[sample], ndmin=2).T     # one-hot-encoding과 같은 2차원 배열이 아닌, 튜플이라던가 다른 자료형이 입력될 경우에 대비해서 전처리 해주기
            Z1, A1, Z2, A2 = self.forpass(A0)       # 순전파 수행
            cost = self.CEcost(A2, Y0)              # 교차 엔트로피 함수 적용
            self.cost_.append(cost)

            # 역전파 수행
            E2 = Y0 - A2                            # Y에 one-hot-encoding을 이미 적용해뒀기 때문에 연산이 수월함
            dZ2 = E2
            dW2 = np.dot(dZ2, A1.T) / self.m_samples
            db2 = np.sum(dZ2, axis=1, keepdims=True) / self.m_samples
            E1 = np.dot(self.W2.T, E2)
            dZ1 = E1 * self.g_prime(Z1)             # 시그모이드 함수 적용
            
            # 또는 쌍곡탄젠트 적용
            # dZ1 = E1 * (1-np.power(A1, 2))

            dW1 = np.dot(dZ1, A0.T)
            db1 = np.sum(dZ1, axis=1, keepdims=True)
            
            # 가중치 조정값 반영
            self.W1 += self.eta * dW1
            self.b1 += self.eta * db1
            self.W2 += self.eta * dW2
            self.b2 += self.eta * db2
    return self

# 검증을 위한 테스트 데이터셋 준비
(X, y), (X_test, y_test) = mlmodule.load_mnist()    # 전처리용 모듈을 사용해서 MNIST 데이터셋을 가져옴
self_accuracy = []
test_accuracy = []

# epoch를 늘려가며 로지스틱 회귀분석 진행
epoch_list = np.arange(1, 31)        
for e in epoch_list:
    nn = LogisticNeuronStochastic_MNIST(784, 100, 10, eta=0.2, epochs=e)
    nn.fit(X, y)
    self_accuracy.append(nn.evaluate(X, y))
    test_accuracy.append(nn.evaluate(X_test, y_test))
</code></pre>
<h2>14-4. Batch Gradient Descent</h2>
<ul>
<li>우리가 배운 경사하강법은 모든 자료의 오차를 합한 것으로부터 가중치를 일괄적으로(batch) 조정하는 방식임
<ul>
<li>모든 샘플들의 오차 총합을 통해, 가중치를 조정하고, 이를 반복해서 오차를 줄임
<ul>
<li>w<sub>new</sub> = w<sub>old</sub> + Δw = w<sub>old</sub> + {η × Σᵢ(y⁽ⁱ⁾ - h(z⁽ⁱ⁾)) × xⱼ⁽ⁱ⁾}
<ul>
<li>w<sub>new</sub> = w<sub>old</sub> + η × 1/m × Σᵢ{(y⁽ⁱ⁾ - ŷ⁽ⁱ⁾) × xⱼ⁽ⁱ⁾}</li>
</ul>
</li>
</ul>
</li>
<li>장점
<ul>
<li>오차가 안정적으로 감소하며 학습됨
<ul>
<li>오차함수의 최소 값으로 수렴할 가능성이 높음</li>
</ul>
</li>
</ul>
</li>
<li>단점
<ul>
<li>데이터셋의 크기가 큰 경우, 가중치를 한 번 조정하려고 수많은 자료의 모든 오차를 계산해야 하는 부담이 발생함
<ul>
<li>메모리 공간이 많이 필요함</li>
<li>훈련 속도가 느려짐</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>배치 경사하강법을 사용한 로지스틱 회귀분석 Code Example</h3>
<pre><code>import mlmodule                                        # 이번 실습을 위해 미리 만들어놓은 전처리용 모듈이 있다고 가정함
import numpy as np

# 데이터셋 준비
(X, y), (X_test, y_test) = mlmodule.load_mnist()       # 전처리용 모듈을 사용해서 MNIST 데이터셋을 가져옴
self_accuracy = []
test_accuracy = []

# 데이터셋의 일부만 slicing 해서 시도해보기
X, y = X[:1000], y[:1000]
X_test, y_test = X_test[:100], ytest[:100]

# fit 메소드 정의
def fit(self, X, y):
    self.cost_ = []
    m_samples = len(y)
    Y = mlmodule.one_hot_encoding(y, self.n_y)
    for epoch in range(self.epochs):
        A0 = np.array(X[sample], ndmin=2).T            # one-hot-encoding과 같은 2차원 배열이 아닌, 튜플이라던가 다른 자료형이 입력될 경우에 대비해서 전처리 해주기
        Y0 = np.array(Y[sample], ndmin=2).T            # one-hot-encoding과 같은 2차원 배열이 아닌, 튜플이라던가 다른 자료형이 입력될 경우에 대비해서 전처리 해주기
        
        # 순전파 수행
        Z1 = np.dot(self.W1, A0)
        A1 = self.g(Z1)
        Z2 = np.dot(self.W2, A1)
        A2 = self.g(Z2)

        # 역전파 수행
        E2 = Y0 - A2                                   # Y에 one-hot-encoding을 이미 적용해뒀기 때문에 연산이 수월함
        E1 = np.dot(self.W2.T, E2)
        dZ2 = E2 * self.g_prime(Z2)
        dZ1 = E1 * self.g_prime(Z1)
        dW2 = self.eta * np.dot(dZ2, A1.T)
        dW1 = self.eta * np.dot(dZ1, A0.T)
            
        # 가중치 조정값 반영
        self.W2 += dW2 / m_samples                     # 배치 경사하강법에서는 오차를 일괄적으로 몰아서 계산하므로 가중치 조정값을 샘플 수 만큼 나누어주어야 함
        self.W1 += dW1 / m_samples                     # 배치 경사하강법에서는 오차를 일괄적으로 몰아서 계산하므로 가중치 조정값을 샘플 수 만큼 나누어주어야 함
        self.cost_.append(np.sqrt(np.sum(E2*E2)))
    
    return self

# 정확도를 측정 및 점검하기 위한 evaluate 메소드 정의
def evaluate(self, X_test, y_test):
    m_samples = len(y_test)
    scores = 0
    A2 = self.predict(X_test)                          # 신경망의 예측값인 A2를 가져오며, A2의 형상은 (10, 100)

    # 테스트 데이터 y_test의 형상은 1차원인 (100, )으로써 A2와 형상이 다르므로,
    # 다차원 배열에서 각 차원에서 가장 큰 값들의 인덱스를 반환해주는 argmax 함수를 사용해서
    # A2의 형상을 y_test의 형상과 같게 맞추어 준 것이 바로 y_hat
    y_hat = np.argmax(A2, axis=0)                      

    # y_hat은 A2를 y_test의 형상과 같게끔 만들어준 것이므로,
    # 비로소 y_test와의 일대일 비교가 가능해졌고, 
    # y_hat과 y_test의 대조를 통해 정확도를 측정함
    scores += np.sum(y_hat == y_test)   

    # 정확도 측정값을 백분율로 환산하여 출력함
    return scores/m_samples * 100

nn = MnistBatchGD(784, 100, 10, eta=0.1, epochs=1000)
nn.fit(X, y)
accuracy = nn.evaluate(X_test, y_test)
print(accuracy)                                         # 정확도를 출력함
</code></pre>
<h2>14-5. Stochastic Gradient Descent</h2>
<ul>
<li>확률적 경사하강법은 배치 경사하강법의 단점을 보완하여 경사를 조금 더 빠르게 수렴시키는 방법임
<ul>
<li>일괄적으로 오차를 한꺼번에 계산해야 하는 배치 경사하강법과 달리, 학습자료 하나를 처리할 때마다 오차를 각각 계산해서 가중치를 바로바로 조정함
<ul>
<li>w<sub>new</sub> = w<sub>old</sub> + Δw = w<sub>old</sub> + {η × (y⁽ⁱ⁾ - h(z⁽ⁱ⁾)) × xⱼ⁽ⁱ⁾}
<ul>
<li>w<sub>new</sub> = w<sub>old</sub> + η × (y⁽ⁱ⁾ - ŷ⁽ⁱ⁾) × xⱼ⁽ⁱ⁾</li>
<li>배치 경사하강법에서 볼 수 있었던 Σ가 없어짐</li>
</ul>
</li>
</ul>
</li>
<li>학습 자료가 불확실한 확률(stochastic)로 입력되고 있다고 가정함
<ul>
<li>입력 자료를 무작위로 섞어줄 필요가 없음</li>
<li>다만 shuffle 해도 상관은 없음</li>
</ul>
</li>
<li>장점
<ul>
<li>메모리 공간이 부족해질 우려가 적음</li>
<li>수렴 속도가 빠름</li>
</ul>
</li>
<li>단점
<ul>
<li>불확실한 확률에 기반하기 때문에 수렴이 불안정함</li>
</ul>
</li>
<li>이러한 장단점으로 인해 대규모 기계학습에서 주로 사용</li>
</ul>
</li>
</ul>
<h3>확률적 경사하강법을 사용한 로지스틱 회귀분석 Code Example</h3>
<pre><code>import mlmodule                                            # 이번 실습을 위해 미리 만들어놓은 전처리용 모듈이 있다고 가정함
import numpy as np

# 데이터셋 준비
(X, y), (X_test, y_test) = mlmodule.load_mnist()           # 전처리용 모듈을 사용해서 MNIST 데이터셋을 가져옴
self_accuracy = []
test_accuracy = []

# 데이터셋의 일부만 slicing 해서 시도해보기
X, y = X[:1000], y[:1000]                                  
X_test, y_test = X_test[:100], ytest[:100] 

# fit 메소드에서 차이가 있음
def fit(self, X, y):
    self.cost_ = []
    m_samples = len(y)
    Y = mlmodule.one_hot_encoding(y, self.n_y)
    for epoch in range(self.epochs):
        for m in range(m_samples):                         # 일괄적으로 한번에 몰아서 계산하는 대신, 샘플 개수 m 횟수만큼 반복함
            A0 = np.array(X[m], ndmin=2).T                 # 샘플 전체가 아닌 하나씩만 가져옴
            Y0 = np.array(Y[m], ndmin=2).T                 # 샘플 전체가 아닌 하나씩만 가져옴
            
            # 순전파 수행
            Z1 = np.dot(self.W1, A0)
            A1 = self.g(Z1)
            Z2 = np.dot(self.W2, A1)
            A2 = self.g(Z2)

            # 역전파 수행
            E2 = Y0 - A2                                   # Y에 one-hot-encoding을 이미 적용해뒀기 때문에 연산이 수월함
            E1 = np.dot(self.W2.T, E2)
            dZ2 = E2 * self.g_prime(Z2)
            dZ1 = E1 * self.g_prime(Z1)
            dW2 = np.dot(dZ2, A1.T)                        # 일괄적으로 계산하는 방식이 아니므로, 학습률은 조정값을 반영할 때 적용시켜주면 됨
            dW1 = np.dot(dZ1, A0.T)                        # 일괄적으로 계산하는 방식이 아니므로, 학습률은 조정값을 반영할 때 적용시켜주면 됨
                
            # 가중치 조정값 반영
            self.W2 += self.eta * dW2                      # 일괄적으로 계산하는 방식이 아니므로, 샘플 개수만큼 나눠줄 필요 없이 바로바로 가중치를 조정함
            self.W1 += self.eta * dW1                      # 일괄적으로 계산하는 방식이 아니므로, 샘플 개수만큼 나눠줄 필요 없이 바로바로 가중치를 조정함
            self.cost_.append(np.sqrt(np.sum(E2*E2)))
    
    return self

# 정확도를 측정 및 점검하기 위한 evaluate 메소드 정의
def evaluate(self, X_test, y_test):
    m_samples = len(y_test)
    scores = 0
    A2 = self.predict(X_test)                              # 신경망의 예측값인 A2를 가져오며, A2의 형상은 (10, 100)

    # 테스트 데이터 y_test의 형상은 1차원인 (100, )으로써 A2와 형상이 다르므로,
    # 다차원 배열에서 각 차원에서 가장 큰 값들의 인덱스를 반환해주는 argmax 함수를 사용해서
    # A2의 형상을 y_test의 형상과 같게 맞추어 준 것이 바로 y_hat
    y_hat = np.argmax(A2, axis=0)                      

    # y_hat은 A2를 y_test의 형상과 같게끔 만들어준 것이므로,
    # 비로소 y_test와의 일대일 비교가 가능해졌고, 
    # y_hat과 y_test의 대조를 통해 정확도를 측정함
    scores += np.sum(y_hat == y_test)

    # 정확도 측정값을 백분율로 환산하여 출력함
    return scores/m_samples * 100

nn = MnistStochasticGD(784, 100, 10, eta=0.1, epochs=20)
nn.fit(X, y)
accuracy = nn.evaluate(X_test, y_test)
print(accuracy)                                            # 정확도를 출력함
</code></pre>
<h2>14-6. Mini-batch Gradient Descent</h2>
<ul>
<li>배치 경사하강법과 확률적 경사하강법의 절충안
<ul>
<li>배치 경사하강법: 모든 샘플의 오차 총합을 한꺼번에 구함</li>
<li>확률적 경사하강법: 각 샘플의 오차를 매번 구함</li>
<li>미니배치 경사하강법: 소규모의 샘플 집합(미니배치)의 오차 총합을 구하는 방식
<ul>
<li>미니배치의 크기(batch size)만큼 샘플들을 임의로 묶어 그 오차를 계산함
<ul>
<li>미니배치의 크기인 bs는 주로 8, 16, 32, 64와 같은 배수로 설정함</li>
</ul>
</li>
<li>행렬 연산에 최적화된 GPU와 Numpy를 활용해서 성능을 향상시킬 수 있음</li>
</ul>
</li>
<li>w<sub>new</sub> = w<sub>old</sub> + η × 1/bs × Σᵢ{(y⁽ⁱ⁾ - ŷ⁽ⁱ⁾) × xⱼ⁽ⁱ⁾}</li>
</ul>
</li>
<li>학습 스케줄
<ul>
<li>배치 경사하강법은 전역 최소값에 바로 도달하는 대신, 연산 속도가 느림</li>
<li>확률적 경사하강법과 미니배치 경사하강법은 속도가 빠르고, 최저점을 향해 이리저리 배회하는 움직임으로 인해 지역 최소값을 건너뛸 수 있다는 장점이 있으나, 대신 전역 최소값을 지나칠 가능성 역시 높음
<ul>
<li>학습을 시작하는 초기에는 학습률을 크게 설정(big step)하고, 이후에는 학습률을 낮춰서(small step), 지역 최소값이 나올만한 초반에는 크게 움직여 지나쳐버리고, 전역 최소값이 있을만한 후반에는 작게 움직여 전역 최소값을 놓치지 않도록 하는 방식으로 이 문제를 해결할 수 있음
<ul>
<li>이를 '학습 스케줄링'이라고 함</li>
</ul>
</li>
</ul>
</li>
<li>&quot;그렇다면 학습률을 어떻게, 얼마나 빠르게/느리게 조절해야 할까?&quot;
<ul>
<li>고정 학습 스케줄링
<ul>
<li>미리 지정된 임의의 학습률로 학습률을 감소시키기
<ul>
<li>e.g. 학습률을 1부터 0.0001까지 에폭 수 2000에 비례 배분해서 감소시키기</li>
</ul>
</li>
</ul>
</li>
<li>성능 기반 학습 스케줄링
<ul>
<li>일정 스텝마다 검증 오차를 측정하고, 오차가 줄어들지 않으면 그때 일정한 크기로 학습률을 감소시키기</li>
</ul>
</li>
<li>지수 기반 학습 스케줄링
<ul>
<li>반복 횟수 t에 기반한 함수로 학습률을 설정하기
<ul>
<li>η(t) = n₀10⁻ᵗᐟʳ
<ul>
<li>학습률은 매 r 스텝마다 줄어듦</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>거듭제곱 기반 학습 스케줄링
<ul>
<li>η(t) = n₀(1 + t/r)⁻ᶜ</li>
</ul>
</li>
</ul>
</li>
<li>고정 학습 스케줄링 code example<pre><code>def fit(self, X, y):
    self.cost_ = []
    m_samples = len(y)
    Y = mlmodule.one_hot_encoding(y, self.n_y)

    # 학습 스케줄링 구현
    # 학습률을 적절한 고정된 간격으로 미리 나눠주고, 나중에 가중치를 조정할 때 이 학습률을 사용함
    eta_scheduled = np.linspace(self.eta, 0.0001, self.epochs)

    for epoch in range(self.epochs):
        A0 = np.array(X[sample], ndmin=2).T                                 # one-hot-encoding과 같은 2차원 배열이 아닌, 튜플이라던가 다른 자료형이 입력될 경우에 대비해서 전처리 해주기
        Y0 = np.array(Y[sample], ndmin=2).T                                 # one-hot-encoding과 같은 2차원 배열이 아닌, 튜플이라던가 다른 자료형이 입력될 경우에 대비해서 전처리 해주기
        
        Z1, A1, Z2, A2 = self.forpass(A0)                                   # 순전파 수행

        # 역전파 수행
        E2 = Y0 - A2                                                        # Y에 one-hot-encoding을 이미 적용해뒀기 때문에 연산이 수월함
        E1 = np.dot(self.W2.T, E2)
        
        dZ2 = E2 * self.g_prime(Z2)
        dZ1 = E1 * self.g_prime(Z1)
        
        # 학습 스케줄링을 사용하고자 하므로,
        # 고정된 학습률을 사용하는 이 단계는 아래의 코드로 대체함
        # dW2 = self.eta * np.dot(dZ2, A1.T)
        # dW1 = self.eta * np.dot(dZ1, A0.T)
        
        # 위에서 학습 스케줄링으로 구현한 학습률을 사용해서 가중치 조정값 반영
        eta = eta_scheduled[epoch]

        self.W2 += eta * np.dot(dZ2, A1.T) / m_samples                      # 배치 경사하강법에서는 오차를 일괄적으로 몰아서 계산하므로 가중치 조정값을 샘플 수 만큼 나누어주어야 함
        self.W1 += eta * np.dot(dZ1, A0.T) / m_samples                      # 배치 경사하강법에서는 오차를 일괄적으로 몰아서 계산하므로 가중치 조정값을 샘플 수 만큼 나누어주어야 함
        self.cost_.append(np.sqrt(np.sum(E2*E2)))
    
    return self
</code></pre>
</li>
</ul>
</li>
</ul>
<h3>미니배치 경사하강법 code example</h3>
<pre><code>def fit(self, X, y):
    self.cost_ = []
    m_samples = len(y)
    Y = mlmodule.one_hot_encoding(y, self.n_y)
    for epoch in range(self.epochs):
        for i in range(0, m_samples, self.batch_size):                          # batch size를 지정해서 그만큼씩 반복해서 계산해나감

            # 순전파 수행
            A0 = X[i: i + self.batch_size]                                      # 입력으로 들어온 전체 샘플을 batch size만큼 슬라이싱
            Y0 = Y[i: i + self.batch_size]                                      # 입력으로 들어온 전체 샘플을 batch size만큼 슬라이싱
            A0 = np.array(X[m], ndmin=2).T
            Y0 = np.array(Y[m], ndmin=2).T
            Z1 = np.dot(self.W1, A0)
            A1 = self.g(Z1)
            Z2 = np.dot(self.W2, A1)
            A2 = self.g(Z2)

            # 역전파 수행
            E2 = Y0 - A2
            E1 = np.dot(self.W2.T, E2)
            dZ2 = E2 * self.g_prime(Z2)
            dZ1 = E1 * self.g_prime(Z1)
            dW2 = np.dot(dZ2, A1.T)
            dW1 = np.dot(dZ1, A0.T)
            
            # 가중치 조정값 반영
            self.W2 += self.eta * dW2 / self.batch_size                         # 전체 샘플 수 대신 batch size 만큼 나누어주어야 함
            self.W1 += self.eta * dW1 / self.batch_size                         # 전체 샘플 수 대신 batch size 만큼 나누어주어야 함
            self.cost_.append(np.sqrt(np.sum(E2*E2))/self.batch_size)
    return self
</code></pre>
<h2>14-7. Overfitting</h2>
<ul>
<li>이진 분류 작업에 미니배치 경사하강법을 적용해서 기계학습을 수행한다고 가정해보기
<ul>
<li>학습이 반복될수록, 학습자료에 대한 분류 정확도가 높아지면서, 테스트 자료에 대한 분류 정확도 역시 함께 높아져야 바람직함</li>
<li>나머지 하이퍼파라미터들을 고정시키고, 반복 횟수만 서서히 높이면 어떻게 될까?
<ul>
<li>학습자료에 대한 분류 정확도는 꾸준히 높아지지만, 테스트 자료에 대한 분류 정확도는 어느정도 높아지다가 오히려 확 감소해버림</li>
</ul>
</li>
<li>나머지 하이퍼파라미터들을 고정시키고, 은닉층 노드의 수만 서서히 높이면 어떻게 될까?
<ul>
<li>학습자료에 대한 분류 정확도는 처음에는 높아지다가 이후 불안정한 모습을 보이며, 테스트 자료에 대한 분류 정확도도 비슷한 추세를 보이나 일부 구간에서는 오히려 감소</li>
</ul>
</li>
<li>학습자료에 대한 분류 정확도는 오르지만 테스트 자료에 대한 분류 정확도가 감소하는 과대적합 구간이 관측됨</li>
</ul>
</li>
</ul>
<h3>과대적합 문제를 해결하기</h3>
<ul>
<li>
<p>Early stopping</p>
<ul>
<li>에폭이 진행될수록 테스트 자료에 대한 정확도가 올라가다가 갑자기 반대로 내려가는 추세가 관측되는 바로 그 지점에, 학습을 중지해버리고 그 때의 가중치를 사용하는 것이 바로 '조기종료'
<ul>
<li>배치 경사하강법의 경우에는 이러한 지점을 찾기가 비교적 쉬움</li>
<li>확률적 경사하강법이나 미니배치 경사하강법의 경우에는 전역 최소값을 찾는 움직임(정확도)의 그래프가 불규칙하게 나오므로 조기종료 지점을 비교적 찾기 어려움</li>
</ul>
</li>
<li>MNIST 데이터셋 분류에 조기 종료를 구현한 code example<pre><code>import mlmodule                                                         # 이번 실습을 위해 미리 만들어놓은 전처리용 모듈이 있다고 가정함

(X, y), (Xtest, ytest) = mlmodule.load_mnist()
trainlist = []
testlist = []
w1 = []
w2 = []

for h1 in range (40, 810, 10):                                          # 은닉층을 늘려감
    nn = MnistMiniBatchGD([784, h1, 10], epochs=20, batch_size=32)
    nn.fit(X[:1000], y[:1000])
    training = nn.evaluate(X[:1000], y[:1000])                          # 학습 자료에 대한 정확도
    test = nn.evaluate(Xtest[:1000], ytest[:1000])                      # 테스트 자료에 대한 정확도

    # 은닉층이 늘어날 때마다 학습 자료와 테스트 자료에 대한 정확도를 기록해나감
    trainlist.append(round(training, 2))
    testlist.append(round(test, 2))

    w1.append(nn.W1)
    w2.append(nn.W2)
</code></pre>
<ul>
<li>은닉층 수를 x축으로, testlist의 값을 y축으로 하는 그래프를 그려보면 어느 시점에서부터는 상승추세가 아닌 하강추세를 보인다는 점을 알 수 있음
<ul>
<li>추세가 반전되는 해당 지점이 바로 과대적합 문제가 발생하는 지점이며, 해당 지점에서 학습을 조기 종료시켜야 함 (은닉층 수를 더 이상 늘리지 않기)</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Data augmentation</p>
<ul>
<li>기존의 학습 데이터를 조금씩 변조해서 양을 늘리기
<ul>
<li>데이터의 패턴을 보다 불규칙하게 만들어 과대적합을 방지하는 방식</li>
</ul>
</li>
<li>데이터 증식은 주로 이미지 데이터셋에서 활용됨
<ul>
<li>e.g.
<ul>
<li>이미지를 약간의 각도로 회전하기</li>
<li>이미지에 노이즈를 일부 추가하기</li>
</ul>
</li>
<li>텍스트를 번역하고 이를 재번역하는 back translation도 데이터 증식의 예시</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Drop out</p>
<ul>
<li>2012년에 컴퓨터과학자 Geoffrey Hinton 교수가 고안한 개념
<ul>
<li>1~2% 정도의 추가적인 정확도 향상을 기대할 수 있음</li>
</ul>
</li>
<li>학습 과정에서 일부 노드/뉴런들을 무작위로 제외하고 나머지 노드들로만 학습을 진행하기
<ul>
<li>학습이 이루어지는 노드와 가중치가 달라짐
<ul>
<li>노드의 학습이 이웃 뉴런에 맞추어 적응(의존)해버리는 부작용을 제거함</li>
<li>각 노드 자체의 독립적인 학습 성능을 높이기</li>
</ul>
</li>
</ul>
</li>
<li>알고리즘이 간단함
<ul>
<li>학습 과정에서는 일정한 비율(일반적으로 약 50%의 확률)로 노드를 drop시키고, 이후 학습이 끝나면 모든 노드를 사용함</li>
<li>드롭아웃 code example<pre><code>def forpass(self, A0, train=True):                                  # 순전파 메소드 구현
    Z1 = np.dot(self.W1, A0)
    A1 = self.g(Z1)

    # 드롭아웃 구현
    if train:
        self.drop_units = \                                         # 학습에 투입하지 않을 뉴런을 임의로 정하기 위한 변수
            np.random.rand(*A1.shape) &gt; self.dropout_ratio          # A1과 같은 형상으로 랜덤값을 생성 후, 그것을 드랍 확률과 비교한 값을 변수에 할당함
        A1 = A1 * self.drop_units / self.dropout_ratio              # 뉴런을 드랍시킴

    Z2 = np.dot(self.W2, A1)
    A2 = self.g(Z2)
    return Z1, A1, Z2, A2
</code></pre>
<ul>
<li>역전파 과정에서도 드롭아웃을 구현해야 함
<ul>
<li>다만 이미 순전파 과정에서 드랍이 된 뉴런은 역전파에서도 사용할 일이 없으므로 코드는 간단해짐
<ul>
<li><code>self.drop_units</code>를 그대로 사용하면 됨</li>
</ul>
</li>
<li>code example<pre><code>dZ1 = dZ1 * self.drop_units
</code></pre>
</li>
</ul>
</li>
<li>평가나 예측 단계에서는 드롭아웃 없이 모든 뉴런이 참여해야 함
<ul>
<li>code example<pre><code>def predict(self, X):
    A0 = np.array(X, ndmin=2).T
    Z1, A1, Z2, A2 = self.forpass(A0, train=False)          # train 매개변수를 False로 설정해서 드롭아웃을 꺼주면 됨
    return A2
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<h1>15. Deep Neural Network (DNN)</h1>
<ul>
<li>은닉층이 여러개인 인공 신경망</li>
<li>코드 구조
<ul>
<li>DeepNeuralNet 클래스
<ul>
<li><code>__init__</code> 메소드</li>
<li>forpass 메소드 = 순전파 과정</li>
<li>backprop 메소드 = 역전파 과정 (fit 메소드로부터 별도로 떼어냈음)</li>
<li>fit 메소드</li>
<li>predict 메소드</li>
<li>evaluate 메소드</li>
</ul>
</li>
<li>활성화 함수
<ul>
<li>tanh 메소드 = 쌍곡탄젠트 함수<pre><code>def tanh(x):
    return (1.0 - np.exp(-2*x))/(1.0 + np.exp(-2*x))
</code></pre>
</li>
<li>tanh_d 메소드 = 쌍곡탄젠트 함수를 미분함<pre><code>def tahn_d(x):
    return (1 + tahn(x)) * (1 - tanh(x))
</code></pre>
</li>
<li>sigmoid 메소드 = 시그모이드 함수<pre><code>def sigmoid(x):
    return 1 / (1 + np.exp((-x)))
</code></pre>
</li>
<li>sigmoid_d 메소드 = 시그모이드 함수를 미분함<pre><code>def sigmoid_d(x):
    return sigmoid(x) * (1 - sigmoid(x))
</code></pre>
</li>
<li>relu 메소드 = ReLU 함수<pre><code>def relu(x):
    return np.maximum(x, 0)
</code></pre>
</li>
<li>relu_d 메소드 = ReLU 함수를 미분함<pre><code>def relu_d(x):
    x[x&lt;=0] = 0
    x[x&gt;0] = 1
    return x
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>15-1. 딥 뉴럴 네트워크 Code Example</h2>
<pre><code>import numpy as np
import mlmodule                                                                         # 이번 실습을 위해 미리 만들어놓은 전처리용 모듈이 있다고 가정함

class DeepNeuralNet(): 

    def __init__(self, net_arch, activate=None, eta=1.0, epochs=100, random_seed = 1):
        self.eta = eta
        self.epochs = epochs
        self.net_arch = net_arch
        self.layers = len(net_arch)
        self.W = []

        # 각 은닉층에 적용할 활성화 함수들을 지정해 인스턴스 변수로 저장해놓기
        # 여기서는 일단 아래의 코드를 사용해서 모든 은닉층에 시그모이드 함수를 적용하도록 함
        self.g =       [lambda x: sigmoid(x)   for _ in range(self.layers)]                     
        self.g_prime = [lambda x: sigmoid_d(x) for _ in range(self.layers)]

        if activate is not None:                                                        # 활성화 과정이 존재할 경우, 활성화 함수들의 목록이 activate 매개변수로 넘어오게 됨
            for i, (g, g_prime) in enumerate(zip(activate[::2], activate[1::2])):       # 그러면 짝수층과 홀수층의 은닉층에 각각 '활성화 함수'와 '미분된 활성화 함수'를 번갈아 적용하기
                self.g[i+1] = g
                self.g_prime[i+1] = g_prime

        # 각 층의 가중치를 랜덤하게 설정하기
        np.random.seed(random_seed)
        self.W = [[None]]
        for layer in range(self.layers - 1):
            w = 2 * np.random.rand(self.net_arch[layer+1], self.net_arch[layer]) - 1    # 가중치 배열의 크기 = 두 층 사이의 노드의 수 = 뒷층의 노드 수 × 앞층의 노드 수
            self.W.append(w)

    def fit(self, X, y):
        self.cost_ = []
        for epoch in range(self.epochs):    
            Z, A = self.forpass(X)                                                      # 입력값을 받아 순전파 과정 진행
            cost = self.backprop(Z, A, y)                                               # 역전파 과정을 통해 가중치를 조정
            self.cost_.append(np.sqrt(np.sum(cost*cost)))                               # 딥 뉴럴 네트워크가 잘 작동하는지 확인하기 위해서 비용을 기록함
        return self
    
    def forpass(self, A0):
        Z = [[None]]                                                                    # 가중치와 입력값을 곱한 순입력 Z (입력층의 순입력은 사용하지 않으므로 None으로 시작함)
        A = []                                                                          # 순입력 Z에 활성화 함수를 적용한 출력 A
        A.append(A0)                                                                    # 입력층의 출력값인 A0를 A 배열에 추가하여 초기화하기
        for i in range(1, len(self.W)):                                                 # 은닉층의 수만큼 순전파 과정을 반복시키고 그 값을 저장해두기
            z = np.dot(self.W[i], A[i-1])                                               
            Z.append(z)
            a = self.g[i](z)
            A.append(a)
        return Z, A
    
    def backprop(self, Z, A, Y):
        E = [None for x in range(self.layers)]                                          # 층 개수만큼 None을 넣어 초기화하기
        dZ = [None for x in range(self.layers)]                                         # 층 개수만큼 None을 넣어 초기화하기

        # 마지막 은닉층으로부터 역전파 시작
        ll = self.layers - 1                                                            # 맨 마지막 은닉층의 위치를 가리키는 인덱스로써 ll 변수를 사용하기로 함
        error = Y - A[ll]                                                               # 은닉층의 마지막 에러값을 계산하기
        E[ll] = error                                                                   
        dZ[ll] = error * self.g_prime[ll](Z[ll])

        for i in range(self.layers-2, 0, -1):                                           # 이후 각 은닉층들에서의 가중치 조정값을 계산하기
            E[i] = np.dot(self.W[i+1].T, E[i+1])                                        # i번째 은닉층에 대한 에러값 계산
            dZ[i] = E[i] * self.g_prime[i](Z[i])                                        # i번째 은닉층에 대한 가중치 조정값 계산
        
        m = Y.shape[0]                                                                  # m은 샘플의 개수
        for i in range(ll, 0, -1):                                                      # 계산된 가중치 조정값을 실제로 적용하기
            self.W[i] += self.eta * np.dot(dZ[i], A[i-1].T) / m

        return error

# XOR 입력에 대해서 딥 뉴럴 네트워크가 잘 작동하는지 확인해보기
X = np.array([[0,0,1,1], [0,1,0,1]])
y = np.array([0,1,1,0])
dnn = DeepNeuralNet([2,4,2,1], eta=0.5, epochs=5000).fit(X, y)                          # 입력층 2개 - 은닉층 4개 - 은닉층 2개 - 출력층 1개로 구성된 딥 뉴럴 네트워크로 학습 시작
</code></pre>
<h3>배치 경사하강법을 적용한 DNN Code Example</h3>
<pre><code>def fit(self, X, y):
    self.m_samples = len(y)
    Y = mlmodule.one_hot_encoding(y, self.net_arch[-1])

    for epoch in range(self.epochs):
        A0 = np.array(X, ndmin=2).T
        Y0 = np.array(Y, ndmin=2).T
        Z, A = self.forpass(A0)
        cost = self.backprop(Z, A, Y0)
        self.cost_.append(np.sqrt(np.sum(cost*cost)))

    return self

def predict(self, X):
    Z, A2 = self.forpass(X)
    A2 = np.array(A2[len(A2)-1])
    return A2[-1] &gt; 0.5

def predict_(self, X):                                                  # evaluate()에서 호출해서 사용할 수 있도록 별도로 마련해둔 내부용 메소드
    A0 = np.array(X, ndmin=2).T
    Z, A = self.forpass(A0)
    return A[-1]
</code></pre>
<h2>15-2. Fashion-MNIST dataset</h2>
<ul>
<li>기존 MNIST handwritten digit 데이터셋의 단점
<ul>
<li>학습이 너무 쉬움
<ul>
<li>고전적인 학습 알고리즘을 사용해도 95% 이상의 정확도가 나옴</li>
</ul>
</li>
<li>이미 너무 많이 사용되고 인용됨
<ul>
<li>보다 복잡한 데이터셋의 필요성</li>
</ul>
</li>
<li>최근의 영상처리 메커니즘을 대표할 수 없음
<ul>
<li>영상처리 학습에 실제로 사용할만큼 실용적이지 않음</li>
</ul>
</li>
</ul>
</li>
<li>패션 기업 Zalando의 의류 및 잡화 이미지들을 MNIST handwritten digit 데이터셋과 유사한 형식으로 가공해둔 것
<ul>
<li>가로 세로 28 픽셀 크기의 흑백 이미지</li>
<li>60,000개의 학습용 데이터셋과 10,000개의 테스트 데이터셋으로 구성</li>
<li>https://github.com/zalandoresearch/fashion-mnist</li>
</ul>
</li>
</ul>
<h2>15-3. Andrew Ng 교수의 Cat vs Non-cat 데이터셋</h2>
<ul>
<li>Coursera에서 제공하는 Neural Networks and Deep Learning 과목에서 사용된 '고양이 판별용 학습 데이터'</li>
<li>고양이 분류기 code example<pre><code>import numpy as np
import mlmodule

X, y, Xtest, ytest, classes = mlmodule.load_cat_data()

print(&quot;Train samples / Test samples / Image size&quot;)
print(&quot;{} / {} / {}, {}, {}&quot;.format(X.shape[0], Xtest.shape[0], X.shape[1], X.shape[1], 3))     # 출력값: 209 / 50 / 64, 64, 3

print(&quot;Train X shape / Train Y shape / Test X shape / Test Y shape&quot;)
print(&quot;{} / {} / {} / {}&quot;.format(X.shape, y.shape, Xtest.shape, ytest.shape))                   # 출력값: (209, 64, 64, 3) / (1, 209) / (50, 64, 64, 3) / (1, 50)

# 데이터의 형상을 (샘플의 개수, 특징의 개수)로 바꿔주는 전처리 과정이 필요함
X_flatten = X.reshape(X.shape[0], -1)
Xtest_flatten = Xtest.reshape(Xtest.shape[0], -1)
X = X_flatten / 255                                                                             # 데이터 정규화
Xtest = Xtest_flatten / 255                                                                     # 데이터 정규화

dnn = DeepNeuralNet_BGD([12288, 100, 50, 2], eta=0.3, epochs=164)                               # 은닉층이 2개, 각각 100노드, 50노드
dnn.fit(X, y.flatten())
training = dnn.evaluate(X, y.flatten())                                                         # 학습용 데이터셋으로 검증
testing = dnn.evaluate(Xtest, ytest.flatten())                                                  # 테스트용 데이터셋으로 검증
print(&quot;Training accuracy: {}%&quot;.format(np.round(training, 2)))
print(&quot;Test accuracy: {}%&quot;.format(np.round(testing, 2)))
</code></pre>
</li>
</ul>
<hr>
<h1>16. 기계학습 프레임워크와 심층 신경망</h1>
<ul>
<li>대표적인 오픈소스 머신러닝 프레임워크들
<ul>
<li>TensorFlow
<ul>
<li>코어는 C++로, 인터페이스는 파이썬을 사용함</li>
<li>합성곱 신경망(CNN)과 순환 신경망(RNN)을 구현할 수 있음</li>
<li>CPU와 GPU를 모두 사용할 수 있음</li>
<li>Keras
<ul>
<li>오픈소스 신경망 라이브러리</li>
<li>파이썬 기반</li>
<li>문법이 간단하고 직관적임</li>
<li>다양한 기계학습 프레임워크를 백엔드로 사용해 그 위에서 작동함</li>
<li>TensorFlow에서는 Keras를 통합하여 사용 중</li>
</ul>
</li>
</ul>
</li>
<li>PyTorch
<ul>
<li>빠르고 간결하게 구현할 수 있음</li>
<li>텐서플로우와는 다르게 define-by-run 방식
<ul>
<li>결과를 실시간으로 시각화해서 보여줌</li>
</ul>
</li>
<li>기존 파이썬 라이브러리와의 호환성이 높음
<ul>
<li>bridge를 사용해 NumPy array과 Torch Tensor간의 변환 가능
<ul>
<li>e.g.
<ul>
<li>ndarray → tensor: <code>tensor_test = torch.from_numpy(ndarray_test)</code></li>
<li>ndarray ← tensor: <code>ndarray_test = tensor_test.numpy()</code></li>
</ul>
</li>
<li>단, 변환된 텐서나 배열은 원본을 따로 복사한 별개의 개체가 아닌, 동일한 메모리 위치를 가르키는 개체임</li>
</ul>
</li>
</ul>
</li>
<li>GPU로도 NumPy와 유사한 tensor 연산을 할 수 있음</li>
</ul>
</li>
<li>Scikit-learn</li>
<li>Caffe2</li>
<li>Microsoft Cognitive Toolkit
<ul>
<li>현재는 deprecate됨</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2>16-1. TensorFlow와 Keras를 사용한 MNIST 데이터셋 학습 Code Example</h2>
<pre><code>(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()          # MNIST 데이터셋 불러오기
X_train = X_train.astype('float32')/255                                             # 데이터 정규화
y_train = y_train.astype('float32')/255                                             # 데이터 정규화
y_train = tf.keras.utils.to_categorical(y_train, 10)                                # one-hot-encoding
y_test = tf.keras.utils.to_categorical(y_test, 10)                                  # one-hot-encoding

# 단계별로 층을 쌓아가는 기본적인 인공 신경망 sequential model을 구성하기
model = tf.keras.models.Sequential([                                                # 순차 모델을 정의함
    tf.keras.layers.Flatten(input_shape=X_train.shape[1:]),                         # 입력층 노드 수 설정 = (28, 28) 형상의 데이터를 flatten하기 = 784
    tf.keras.layers.Dense(512, activation='relu'),                                  # 은닉층 노드 수(여기서는 512개) 및 활성화 함수(ReLU) 설정
    tf.keras.layers.Dropout(0.2),                                                   # 과적합 문제를 해결하기 위한 dropout을 0.2 비율로 설정
    tf.keras.layers.Dense(10, activation='softmax')                                 # 출력층 노드 수(0부터 9까지의 숫자 = 10개) 및 활성화 함수(softmax) 설정 
])

model.summary()                                                                     # 신경망 모델을 요약힌 정보를 표시

# 구성한 신경망 모델을 컴파일하기
model.compile(
    loss='categorical_crossentropy',                                                # 손실 함수 지정 (여기서는 교차 엔트로피)
    optimizer='rmsprop',                                                            # 옵티마이저 지정 (여기서는 rmsprop)
    metrics=['accuracy']                                                            # 성능 검증에 사용할 메트릭 기준 지정 (여기서는 정확도)
)

# 학습이 오래 걸릴 경우를 대비해 스냅샷을 저장해주는 체크포인트 설정
# 성능이 향상될 때마다 지정된 파일에다가 가중치를 저장해줌
checkpointer = tf.keras.callbacks.ModelCheckpoint(
    filepath='mnist_best.h5',
    verbose=1,
    save_best_only=True)

# 신경망 모델을 사용해 실제 학습을 진행
model.fit(X_train, y_train, epochs=10,
    batch_size=128,
    validation_split=0.2,
    callbacks=[checkpointer],
    verbose=1,
    shuffle=True)

# 학습과 훈련데이터를 통해 완성된 모델의 가중치를 사용하기
model.load_weights('mnist_best.h5')

# 테스트 데이터셋을 사용해 모델의 성능을 측정하고 그 결과를 출력하기
loss_and_metrics = model.evaluate(X_test, y_test)                                   # evaluate() 메소드는 loss와 metric을 반환함

accuracy = 100 * loss_and_metrics[1]
print(&quot;Accuracy: {}%&quot;.format(accuracy))
</code></pre>
<h2>16-2. Convolution Neural Network</h2>
<ul>
<li>이미지 형식인 MNIST 데이터셋을 ANN에 적용하기 위해서는 (28, 28) 배열인 원본 데이터를 (784, 1) 형상으로 flatten 해야 했음
<ul>
<li>이미지는 가로와 세로로 픽셀이 나열된 2차원 배열임</li>
<li>하지만 기존의 ANN은 2차원 데이터를 입력층에서 그대로 입력받지 못함
<ul>
<li>2차원적인 이미지의 특징을 제대로 반영하지 못함</li>
</ul>
</li>
</ul>
</li>
<li>합성곱 신경망(CNN)은 이미지의 일부분을 2차원 그대로 입력받는 합성곱층(convolutional layer)을 활용함
<ul>
<li>이미지의 일부분인 2차원 영역(filter)을 스캔하고 이를 이미지 전체에 걸쳐서 슬라이딩하면서 계산함
<ul>
<li>입력 데이터에 필터를 먹여서 계산한 결과(feature map)를 모두 합쳐서 합성곱층을 구성함</li>
</ul>
</li>
<li>다만 합성곱층들이 많아지면 과적합이 발생할 확률이 높아짐
<ul>
<li>합성곱층의 차원(dimensionality)을 줄여주는 pooling layer를 활용하면 됨
<ul>
<li>Max pooling layer
<ul>
<li>특정 크기의 지역마다 최대값을 찾아 모으는 방식</li>
</ul>
</li>
<li>Global average pooling layer
<ul>
<li>특정 크기의 지역의 평균값을 계산해서 모으는 방식</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>CNN을 사용한 MNIST 데이터셋 학습 Code Example</h3>
<pre><code>model = Sequential([
    tf.keras.layers.Conv2D(
        kernel_size=2,                                      # 커널 사이즈가 가로 세로 각각 2인
        filters=16,                                         # 필터 16개를 이용함
        padding='valid',
        activation='relu',
        input_shape=(28,28,1)),                             # 입력층의 형상이 2차원인 것을 볼 수 있음
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.MaxPooling2D(pool_size=2),

    tf.keras.layers.Conv2D(
        filters=32,
        kernel_size=2,
        padding='valid',
        activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.MaxPooling2D(pool_size=2),

    tf.keras.layers.Conv2D(
        kernel_size=2, 
        filters=64,
        padding='valid',
        activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.MaxPooling2D(pool_size=2),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(10, acivation='softmax')
])
</code></pre>
<h3>PyTorch를 사용한 CNN 구현 Code Example</h3>
<pre><code>class CNN(nn.Module):                                                   # nn.Module 클래스를 상속받아서 CNN 시작
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=2)                    # 생성자를 통해 합성곱층 세 개를 만들어냄
        self.conv2 = nn.Conv2d(16, 32, kernel_size=2)                   # 입력받은 이미지 전체를 지역으로 나누어
        self.conv3 = nn.Conv2d(32, 64, kernel_size=2)                   # 지역적인 특성을 파악하기
        self.dropout = nn.Dropout(0.2)
        self.fc1 = nn.Linear(256, 10)                                   # 출력층으로 linear transformation 객체를 사용함
    
    def forward(self, x):                                               # 순전파
        
        # 세 개의 합성곱층에 max pooling 방식을 적용하여 
        # 차원을 감소시켜주는 pooling 진행
        x = F.max_pool2d(F.relu(self.conv1(x)), 2)
        x = self.dropout(x) 
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = self.dropout(x)
        x = F.max_pool2d(F.relu(self.conv3(x)), 2)
        x = x.view(-1, 256)
        x = self.fc1(x)
        return F.log_softmax(x, dim=1)

cnn = CNN()

criterion = torch.nn.CrossEntropyLoss()                                 # 교차 엔트로피 손실함수
optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)                # Adam 옵티마이저

print(cnn)                                                              # 구현한 CNN의 정보를 확인하기

# 학습을 시작하기 
epochs = 10
cnn.train()                                                             # CNN을 학습 모드로 전환시키기
for epoch in range(epochs):
    avg_loss = 0
    for X, Y in data_loader:
        optimizer.zero_grad()                                           # 옵티마이저의 gradient를 0으로 초기화
        y_hat = cnn(X)                                                  # 입력값 X를 CNN에 입력해서 결과값을 얻음
        loss = criterion(y_hat, Y)                                      # 결과값인 y_hat과 클래스 레이블 Y를 사용해 loss를 계산
        loss.backward()                                                 # 역전파로 gradient를 계산하기
        optimizer.step()                                                # 파라미터를 업데이트 시키기
        avg_loss += loss.item() / len(data_loader)

    print('|Epoch: {:3d}| Loss: {:.7f}'.format(epoch+1, avg_loss))

# 평가 단계를 시작하기
cnn.eval()                                                              # CNN을 평가 모드로 전환시키기
with torch.no_grad():                                                   # 평가 단계이므로, 학습을 진행하지 않을 것임을 명시해줘야 함
    X_test = mnist_test.data.view(len(mnist_test), 1, 28, 28).float()
    Y_test = mnist_test.targets

    prediction = cnn(X_test)
    correct_prediction = torch.argmax(prediction, 1) == Y_test
    accuracy = correct_prediction.float().mean()
    print('Accuracy: ', accuracy.item())
</code></pre>
<h2>16-3. 기타 기계학습 모델 소개</h2>
<ul>
<li>You Only Look Once (YOLO)
<ul>
<li>실시간 물체인식에 사용됨</li>
<li>1초에 45장의 이미지를 분석할 수 있음 (45fps)</li>
</ul>
</li>
<li>Generative Adversarial Network (GAN)
<ul>
<li>생성형 인공지능에 사용됨</li>
<li>학습된 데이터를 기반으로 이미지를 직접 생성함</li>
<li>Deep Convolutional GAN (dcGAN)</li>
</ul>
</li>
</ul>
<hr>
<p>짧은 시간 안에 숙달하기에는 어려운 내용이지만 한 학기동안 맛깔나게 배웠습니다. 수강하기를 잘했네요. 인공지능과 머신러닝은 학부 수준으로만 이렇게 배워두고, 일단은 급한 웹 프로그래밍 공부부터 먼저 해야겠습니다.</p>

    </div>
    
    <div class="mt-20 md:mt-32 lg:mt-32 xl:mt-32"></div>
</article>
        
    </main>
    <footer class="mt-20 px-10 py-8 bg-gray-200">
    <div class="max-w-5xl mx-auto text-gray-700 text-center">
        © 2022 <a href="/" class="font-medium" target="_blank" rel="noopener">devfreedom.github.io</a> by 
        <a href="https://devfreedom.github.io" target="_blank" rel="noopener">devfreedom</a> 
    </div>
</footer>
    <script src="/assets/js/bundle.js"></script>
</body>

</html>